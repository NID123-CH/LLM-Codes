{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NID123-CH/LLM-Codes/blob/main/06_Exercise_Quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b2830bd",
      "metadata": {
        "id": "1b2830bd"
      },
      "source": [
        "# Exercise: Quantize and Serve a Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c144bff5",
      "metadata": {
        "id": "c144bff5"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate bitsandbytes peft"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7fcceb9",
      "metadata": {
        "id": "e7fcceb9"
      },
      "source": [
        "## Clone llama.cpp (Version 2760) / Install GGUF-PY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3051ee0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3051ee0a",
        "outputId": "17f649cc-38a9-410b-85bc-cd98505de571"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting peft\n",
            "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes, peft\n",
            "Successfully installed bitsandbytes-0.43.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 peft-0.12.0\n",
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 15623, done.\u001b[K\n",
            "remote: Counting objects: 100% (3930/3930), done.\u001b[K\n",
            "remote: Compressing objects: 100% (145/145), done.\u001b[K\n",
            "remote: Total 15623 (delta 3860), reused 3786 (delta 3785), pack-reused 11693\u001b[K\n",
            "Receiving objects: 100% (15623/15623), 21.37 MiB | 17.29 MiB/s, done.\n",
            "Resolving deltas: 100% (11100/11100), done.\n",
            "Note: switching to '3f167476b11efa7ab08f6cacdeb8cab0935c1249'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "Processing /content/llama.cpp/gguf-py\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from gguf==0.9.0) (1.26.4)\n",
            "Building wheels for collected packages: gguf\n",
            "  Building wheel for gguf (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gguf: filename=gguf-0.9.0-py3-none-any.whl size=30130 sha256=05930592d024d926ba203d7ddb7023042ed5dac610997efbd2158ef6543006c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/42/13/ff5427daa7a8a54a9bbcfafc10a0cd0cb13eae7952fb878270\n",
            "Successfully built gguf\n",
            "Installing collected packages: gguf\n",
            "Successfully installed gguf-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!git clone -b b2760 --single-branch https://github.com/ggerganov/llama.cpp.git\n",
        "!cd llama.cpp/gguf-py && pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2399de8",
      "metadata": {
        "id": "e2399de8"
      },
      "source": [
        "## Supported Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dEZPOEBOVVmA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEZPOEBOVVmA",
        "outputId": "3483678b-ae4e-4abc-ca6a-110910d0b37e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<MODEL_ARCH.LLAMA: 1>,\n",
              " <MODEL_ARCH.FALCON: 2>,\n",
              " <MODEL_ARCH.BAICHUAN: 3>,\n",
              " <MODEL_ARCH.GROK: 4>,\n",
              " <MODEL_ARCH.GPT2: 5>,\n",
              " <MODEL_ARCH.GPTJ: 6>,\n",
              " <MODEL_ARCH.GPTNEOX: 7>,\n",
              " <MODEL_ARCH.MPT: 8>,\n",
              " <MODEL_ARCH.STARCODER: 9>,\n",
              " <MODEL_ARCH.PERSIMMON: 10>,\n",
              " <MODEL_ARCH.REFACT: 11>,\n",
              " <MODEL_ARCH.BERT: 12>,\n",
              " <MODEL_ARCH.NOMIC_BERT: 13>,\n",
              " <MODEL_ARCH.BLOOM: 14>,\n",
              " <MODEL_ARCH.STABLELM: 15>,\n",
              " <MODEL_ARCH.QWEN: 16>,\n",
              " <MODEL_ARCH.QWEN2: 17>,\n",
              " <MODEL_ARCH.QWEN2MOE: 18>,\n",
              " <MODEL_ARCH.PHI2: 19>,\n",
              " <MODEL_ARCH.PHI3: 20>,\n",
              " <MODEL_ARCH.PLAMO: 21>,\n",
              " <MODEL_ARCH.CODESHELL: 22>,\n",
              " <MODEL_ARCH.ORION: 23>,\n",
              " <MODEL_ARCH.INTERNLM2: 24>,\n",
              " <MODEL_ARCH.MINICPM: 25>,\n",
              " <MODEL_ARCH.GEMMA: 26>,\n",
              " <MODEL_ARCH.STARCODER2: 27>,\n",
              " <MODEL_ARCH.MAMBA: 28>,\n",
              " <MODEL_ARCH.XVERSE: 29>,\n",
              " <MODEL_ARCH.COMMAND_R: 30>,\n",
              " <MODEL_ARCH.DBRX: 31>,\n",
              " <MODEL_ARCH.OLMO: 32>]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gguf\n",
        "list(gguf.MODEL_ARCH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "517c003d",
      "metadata": {
        "id": "517c003d"
      },
      "source": [
        "## Load Your Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4ecd596",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305,
          "referenced_widgets": [
            "b27fd703ed994a4dad04ffa7204677ca",
            "061fa59d25c64a9584b3e231fd8e91f9",
            "661d989d3d554317a232ebe98f73454f",
            "37db8232247d4620a15737be1e47217c",
            "fec58d1bc38d42c5b4c5f02d2b36ec1c",
            "d5d8809d1bee48a684814e46b776a8f4",
            "4f377e89dffa4abbba1ca2db0d746cc9",
            "a1a06b050fe649f28088176ffb721119",
            "f66502075a9447f88ca8843817e3380b",
            "5c5fc4681099425b8ea3577758f9a940",
            "0f5093a96ac340ce98c1967cf8c65c70",
            "d6039c2a9f7a4f3abe58a50a4c8f11ec",
            "04959f12e8e84724859efc0a119a3a92",
            "d04240f7ca5048829bb888d3de1e6240",
            "550c9fab7d6346c5a2fab6a1d86609ef",
            "3a4a183a5cc2445c969d1e083896bb7e",
            "fb97abf996bc4c7d80d77e1069b013b0",
            "fbd85da9eb32470c9218e78fe4c5743b",
            "f2bf83e6a0af48ac93b5ea258977d6b9",
            "3b04a0ea7c454c6b8dd473137bd495a0",
            "a5873674d95446a0817329aaadd0c771",
            "b060011427fe40f3bfcd9565d8879e10",
            "d9167d8be97b443fb655790037f51657",
            "c1848ff32e88434a9d27bce93e008b30",
            "e11cba9a79ac48f290a14b58d82600de",
            "e3b3b18623214890a0ec08e28d475c2c",
            "a7c6b1fb808342749259e03ecbbfeba4",
            "ca4cd11acb964e0aab3b939d86f7179d",
            "01484c611d2d484bb2f8fc5f10b4bd3d",
            "e92203bcba244e17ab78f9ea7ad5937d",
            "ac730caae04041399041db579827a6aa",
            "89021f1f1c8846648698b25941ccca25",
            "a6d76ca08ed04c418d7d7765e95bb026",
            "14d5f9af3fb44e18ac3d4e7e22059d9c",
            "0f427ce898df4dd7a024ab714bb72eac",
            "924d52e3548647bcad110214542bbfda",
            "484efcf7ca924b688c9e98ae109ebfc1",
            "2cc4c9ead9fc45f8b2c00c3268de0bf3",
            "bb7d7291e67b4f33ba76b6a4b1d27e68",
            "27a244515f7f4cce965e5c2695faede3",
            "48c5105429a140f3bcb676a5be27728f",
            "0b69df942672445bae73504390278833",
            "74fd404e3b6a4a0191bb405ec814d0d8",
            "4c93dfd6c8024ff8a0e5c1f5b3fef58b",
            "cdbb605502614593a621b61251d3446d",
            "2c5a77bb4bc4464b8c2bdb30d09d575e",
            "7768e51462ee42c2b54ce4d5d362756c",
            "f849100d1d5c479ab96432b845bbcf14",
            "04d7c2674bfe4632b4e10e57f9f6af69",
            "b8799e4e6a25484ab02130bff9615692",
            "cb67fe88c1144e0f9dc2c8755d284667",
            "54b7f4ee7e85411c88985ea6b6c051c2",
            "46e4143958c24c1c9be5fb703207a8ad",
            "96ed0bf05fe945faa45155540d7d740f",
            "76b83c075e2440cd80a9eedd452d2b63",
            "3101fcb47d204ea5b76972e194fa6a00",
            "00875a803ab9495c97de53b5329a66d3",
            "61c013cf59ca4729b30d37a3959a4847",
            "f990c9a5128d49b69cb706f4a3e25f61",
            "debf95056f8b44d5b3ff0256982f07a9",
            "0cd7c64331c14e39b31588a22d16967e",
            "43dda54f43a44f5398406e9b3806c5de",
            "86f5539647d845258780e6629ef4aefd",
            "64e9791479be40668aaad4d31b7f6f39",
            "8ad11d7a8a934456a0409a10211529b1",
            "5d02693fea674a42b92f5455f591b54c",
            "552044de39e14a22bbb27b39bb9a337d",
            "c67ba8bdfdf24acdace130683152c38e",
            "606984fe981c462f96d0aca1e52f280a",
            "1abd33e170f8487b89beadbefc5ecc84",
            "e401fc2cbc7a4ac2b7a5d54b8148ceb6",
            "2ee3d2dacf814df3b9ff55d4f1d78e48",
            "4601955d3c10426f9b80244feefb22b4",
            "a664f9dbdce74ef2a1a91fd71a60a5ad",
            "1631c1159cda4d6f8e1f68f847aa6f3f",
            "755130fa95c344b296cb957115f60c05",
            "7dccb1bf733346ad90931f40d15ea6b8",
            "213c83a374f54b0b9530f4e7e74c517c",
            "c0f8669799044a558155ac8cd28eab6b",
            "bc03cc02710e4c4eb1092884cc185648",
            "8ffd8b7eb3de41f0931d32aae0f9ad4a",
            "17bffe95e5d6473cba7ef628144c8a83",
            "048f0ba88f72453fb6e36659674e2c91",
            "98f5bc345bdd4e91b50b0ed5a67df1d7",
            "6d76d349991c46ceafc3523c09ec8be0",
            "6b42acb4c79042fe95e77fa03a28ac6b",
            "8cd90f2249f6450ca9d76f5435f2ac8d",
            "3427805c48cf465f8e57a8953d37c078",
            "dd86f0e7deb84d1dac7a7ca9bd3aa1d1",
            "495b4930606248cfb82694c15df9e689",
            "bc60b400404d44fe9b91af871d74f0b2",
            "f15ccd34d13a4b0a922a0c8437a9befe",
            "24058594877f4d4884c5b714f78dc88f",
            "0b048bf56c96483188eb20c1e41e0f67",
            "3c63eb1d7f8140c984b1c0554b769fe2",
            "55e2eba5b5b84cd1a7b6453733feef90",
            "1d64f21e21df4796b4e0bbef9a8db93d",
            "7f650bb4284e475fb633094f3000114c",
            "25242e51996c4235ba92fac88eff0e50"
          ]
        },
        "id": "c4ecd596",
        "outputId": "423ab285-09b5-446b-d265-5006f4e7305b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b27fd703ed994a4dad04ffa7204677ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6039c2a9f7a4f3abe58a50a4c8f11ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9167d8be97b443fb655790037f51657",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14d5f9af3fb44e18ac3d4e7e22059d9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cdbb605502614593a621b61251d3446d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/68.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3101fcb47d204ea5b76972e194fa6a00",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/197 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "552044de39e14a22bbb27b39bb9a337d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/915 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "213c83a374f54b0b9530f4e7e74c517c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.55G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd86f0e7deb84d1dac7a7ca9bd3aa1d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "name = \"Locutusque/gpt2-large-medical\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(name,\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             trust_remote_code=True,\n",
        "                                             device_map={\"\": 0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QSZWs7BEVzrC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "QSZWs7BEVzrC",
        "outputId": "a1d5cb1a-abce-4aec-9824-38e36eff7247"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel</b><br/>def _wrapped_call_impl(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py</a>The GPT2 Model transformer with a language modeling head on top (linear layer with weights tied to the input\n",
              "embeddings).\n",
              "\n",
              "\n",
              "This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
              "library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n",
              "etc.)\n",
              "\n",
              "This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
              "Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
              "and behavior.\n",
              "\n",
              "Parameters:\n",
              "    config ([`GPT2Config`]): Model configuration class with all the parameters of the model.\n",
              "        Initializing with a config file does not load the weights associated with the model, only the\n",
              "        configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 1284);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.__class__"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88434e2f",
      "metadata": {
        "id": "88434e2f"
      },
      "source": [
        "## Save Model to Disk for Conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b0c8083",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b0c8083",
        "outputId": "2950b056-45a7-4222-b048-9ee24c877449"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./model/tokenizer_config.json',\n",
              " './model/special_tokens_map.json',\n",
              " './model/vocab.json',\n",
              " './model/merges.txt',\n",
              " './model/added_tokens.json',\n",
              " './model/tokenizer.json')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!rm -rf ./model && mkdir model\n",
        "model.save_pretrained('./model')\n",
        "tokenizer.save_pretrained('./model')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4eadd880",
      "metadata": {
        "id": "4eadd880"
      },
      "source": [
        "## Convert Model to GGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40613f53",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40613f53",
        "outputId": "cce4ea24-6235-4932-9977-bf19ff140914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model: model\n",
            "gguf: This GGUF file is for Little Endian only\n",
            "Set model parameters\n",
            "Set model tokenizer\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "gguf: Adding 50000 merge(s).\n",
            "gguf: Setting special token type bos to 50256\n",
            "gguf: Setting special token type eos to 50256\n",
            "gguf: Setting special token type unk to 50256\n",
            "gguf: Setting special token type pad to 50257\n",
            "Exporting model to 'model/ggml-model-f16.gguf'\n",
            "gguf: loading model part 'model.safetensors'\n",
            "blk.0.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.0.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.0.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.0.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.1.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.1.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.1.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.1.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.10.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.10.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.10.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.10.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.11.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.11.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.11.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.11.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.12.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.12.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.12.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.12.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.13.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.13.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.13.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.13.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.14.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.14.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.14.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.14.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.15.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.15.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.15.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.15.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.16.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.16.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.16.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.16.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.17.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.17.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.17.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.17.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.18.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.18.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.18.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.18.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.19.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.19.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.19.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.19.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.2.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.2.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.2.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.2.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.20.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.20.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.20.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.20.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.21.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.21.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.21.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.21.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.22.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.22.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.22.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.22.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.23.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.23.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.23.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.23.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.24.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.24.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.24.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.24.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.24.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.24.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.24.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.24.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.24.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.24.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.24.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.24.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.25.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.25.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.25.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.25.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.25.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.25.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.25.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.25.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.25.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.25.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.25.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.25.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.26.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.26.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.26.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.26.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.26.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.26.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.26.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.26.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.26.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.26.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.26.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.26.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.27.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.27.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.27.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.27.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.27.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.27.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.27.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.27.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.27.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.27.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.27.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.27.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.28.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.28.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.28.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.28.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.28.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.28.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.28.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.28.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.28.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.28.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.28.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.28.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.29.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.29.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.29.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.29.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.29.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.29.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.29.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.29.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.29.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.29.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.29.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.29.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.3.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.3.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.3.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.3.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.30.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.30.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.30.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.30.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.30.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.30.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.30.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.30.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.30.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.30.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.30.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.30.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.31.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.31.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.31.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.31.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.31.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.31.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.31.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.31.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.31.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.31.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.31.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.31.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.32.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.32.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.32.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.32.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.32.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.32.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.32.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.32.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.32.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.32.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.32.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.32.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.33.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.33.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.33.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.33.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.33.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.33.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.33.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.33.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.33.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.33.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.33.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.33.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.34.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.34.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.34.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.34.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.34.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.34.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.34.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.34.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.34.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.34.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.34.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.34.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.35.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.35.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.35.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.35.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.35.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.35.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.35.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.35.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.35.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.35.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.35.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.35.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.4.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.4.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.4.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.4.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.5.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.5.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.5.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.5.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.6.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.6.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.6.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.6.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.7.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.7.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.7.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.7.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.8.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.8.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.8.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.8.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.9.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.9.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.9.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.ffn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.9.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "output_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "output_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "position_embd.weight, n_dims = 2, torch.float16 --> float16\n",
            "token_embd.weight, n_dims = 2, torch.float16 --> float16\n",
            "output.weight, n_dims = 2, torch.float16 --> float16\n",
            "Model successfully exported to 'model/ggml-model-f16.gguf'\n"
          ]
        }
      ],
      "source": [
        "!python ./llama.cpp/convert-hf-to-gguf.py ./model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9470438",
      "metadata": {
        "id": "b9470438"
      },
      "source": [
        "## Builds llama.cpp for Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caeb71f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "caeb71f5",
        "outputId": "cf329a76-55af-4961-e208-728097f67a96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I ccache not found. Consider installing it for faster compilation.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE \n",
            "I NVCCFLAGS: -std=c++11 -O3 \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       c++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "rm -vrf *.o tests/*.o *.so *.a *.dll benchmark-matmult lookup-create lookup-merge lookup-stats common/build-info.cpp *.dot *.gcno tests/*.gcno *.gcda tests/*.gcda *.gcov tests/*.gcov lcov-report gcovr-report main quantize quantize-stats perplexity imatrix embedding vdot q8dot train-text-from-scratch convert-llama2c-to-ggml simple batched batched-bench save-load-state server gguf gguf-split eval-callback llama-bench libllava.a llava-cli baby-llama beam-search retrieval speculative infill tokenize benchmark-matmult parallel finetune export-lora lookahead lookup passkey gritlm tests/test-c.o tests/test-llama-grammar tests/test-grammar-parser tests/test-double-float tests/test-grad0 tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-sampling tests/test-tokenizer-0-llama tests/test-tokenizer-0-falcon tests/test-tokenizer-1-llama tests/test-tokenizer-1-bpe tests/test-rope tests/test-backend-ops tests/test-model-load-cancel tests/test-autorelease tests/test-json-schema-to-grammar tests/test-grammar-integration\n",
            "rm -vrf ggml-cuda/*.o\n",
            "find examples pocs -type f -name \"*.o\" -delete\n",
            "I ccache not found. Consider installing it for faster compilation.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE \n",
            "I NVCCFLAGS: -std=c++11 -O3 \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       c++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c llama.cpp -o llama.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/common.cpp -o common.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/sampling.cpp -o sampling.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/grammar-parser.cpp -o grammar-parser.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/build-info.cpp -o build-info.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/json-schema-to-grammar.cpp -o json-schema-to-grammar.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/console.cpp -o console.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c sgemm.cpp -o sgemm.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c unicode.cpp -o unicode.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c unicode-data.cpp -o unicode-data.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/main/main.cpp -o examples/main/main.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o console.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/main/main.o -o main  \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/quantize/quantize.cpp -o examples/quantize/quantize.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/quantize/quantize.o -o quantize  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/quantize-stats/quantize-stats.cpp -o examples/quantize-stats/quantize-stats.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  build-info.o ggml.o llama.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/quantize-stats/quantize-stats.o -o quantize-stats  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/perplexity/perplexity.cpp -o examples/perplexity/perplexity.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/perplexity/perplexity.o -o perplexity  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/imatrix/imatrix.cpp -o examples/imatrix/imatrix.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/imatrix/imatrix.o -o imatrix  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/embedding/embedding.cpp -o examples/embedding/embedding.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/embedding/embedding.o -o embedding  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c pocs/vdot/vdot.cpp -o pocs/vdot/vdot.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o pocs/vdot/vdot.o -o vdot  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c pocs/vdot/q8dot.cpp -o pocs/vdot/q8dot.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o pocs/vdot/q8dot.o -o q8dot  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/train.cpp -o train.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/train-text-from-scratch/train-text-from-scratch.cpp -o examples/train-text-from-scratch/train-text-from-scratch.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o train.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/train-text-from-scratch/train-text-from-scratch.o -o train-text-from-scratch  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp -o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o -o convert-llama2c-to-ggml  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/simple/simple.cpp -o examples/simple/simple.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/simple/simple.o -o simple  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/batched/batched.cpp -o examples/batched/batched.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/batched/batched.o -o batched  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/batched-bench/batched-bench.cpp -o examples/batched-bench/batched-bench.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  build-info.o ggml.o llama.o common.o sampling.o grammar-parser.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/batched-bench/batched-bench.o -o batched-bench  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/save-load-state/save-load-state.cpp -o examples/save-load-state/save-load-state.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/save-load-state/save-load-state.o -o save-load-state  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/server/server.cpp -o examples/server/server.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o -Iexamples/server examples/server/server.o -o server   \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/gguf/gguf.cpp -o examples/gguf/gguf.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/gguf/gguf.o -o gguf  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/gguf-split/gguf-split.cpp -o examples/gguf-split/gguf-split.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/gguf-split/gguf-split.o -o gguf-split  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/eval-callback/eval-callback.cpp -o examples/eval-callback/eval-callback.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/eval-callback/eval-callback.o -o eval-callback  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/llama-bench/llama-bench.cpp -o examples/llama-bench/llama-bench.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/llama-bench/llama-bench.o -o llama-bench  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/llava/llava-cli.cpp -o examples/llava/llava-cli.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/llava/clip.cpp  -o examples/llava/clip.o -Wno-cast-qual\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/llava/llava.cpp -o examples/llava/llava.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/llava/llava-cli.o examples/llava/clip.o examples/llava/llava.o -o llava-cli  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/baby-llama/baby-llama.cpp -o examples/baby-llama/baby-llama.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o train.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/baby-llama/baby-llama.o -o baby-llama  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/beam-search/beam-search.cpp -o examples/beam-search/beam-search.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/beam-search/beam-search.o -o beam-search  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/retrieval/retrieval.cpp -o examples/retrieval/retrieval.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/retrieval/retrieval.o -o retrieval  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/speculative/speculative.cpp -o examples/speculative/speculative.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/speculative/speculative.o -o speculative  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/infill/infill.cpp -o examples/infill/infill.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o console.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/infill/infill.o -o infill  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/tokenize/tokenize.cpp -o examples/tokenize/tokenize.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/tokenize/tokenize.o -o tokenize  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/benchmark/benchmark-matmult.cpp -o examples/benchmark/benchmark-matmult.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  build-info.o ggml.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/benchmark/benchmark-matmult.o -o benchmark-matmult  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/parallel/parallel.cpp -o examples/parallel/parallel.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/parallel/parallel.o -o parallel  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/finetune/finetune.cpp -o examples/finetune/finetune.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o train.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/finetune/finetune.o -o finetune  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/export-lora/export-lora.cpp -o examples/export-lora/export-lora.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/export-lora/export-lora.o -o export-lora  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/lookahead/lookahead.cpp -o examples/lookahead/lookahead.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookahead/lookahead.o -o lookahead  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/ngram-cache.cpp -o ngram-cache.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup.cpp -o examples/lookup/lookup.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup.o -o lookup  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup-create.cpp -o examples/lookup/lookup-create.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup-create.o -o lookup-create  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup-merge.cpp -o examples/lookup/lookup-merge.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup-merge.o -o lookup-merge  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup-stats.cpp -o examples/lookup/lookup-stats.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup-stats.o -o lookup-stats  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/passkey/passkey.cpp -o examples/passkey/passkey.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/passkey/passkey.o -o passkey  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/gritlm/gritlm.cpp -o examples/gritlm/gritlm.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/gritlm/gritlm.o -o gritlm  \n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n",
            "Collecting numpy~=1.24.4 (from -r llama.cpp/./requirements/requirements-convert.txt (line 1))\n",
            "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: sentencepiece~=0.1.98 in /usr/local/lib/python3.10/dist-packages (from -r llama.cpp/./requirements/requirements-convert.txt (line 2)) (0.1.99)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.35.2 in /usr/local/lib/python3.10/dist-packages (from -r llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.42.4)\n",
            "Requirement already satisfied: gguf>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from -r llama.cpp/./requirements/requirements-convert.txt (line 4)) (0.9.0)\n",
            "Collecting protobuf<5.0.0,>=4.21.0 (from -r llama.cpp/./requirements/requirements-convert.txt (line 5))\n",
            "  Downloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting torch~=2.1.1 (from -r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting einops~=0.7.0 (from -r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 3))\n",
            "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.0.106)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)\n",
            "Collecting triton==2.1.0 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.6.20)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)\n",
            "Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, protobuf, nvidia-nccl-cu12, numpy, einops, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.1\n",
            "    Uninstalling triton-2.3.1:\n",
            "      Successfully uninstalled triton-2.3.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: einops\n",
            "    Found existing installation: einops 0.8.0\n",
            "    Uninstalling einops-0.8.0:\n",
            "      Successfully uninstalled einops-0.8.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.1+cu121\n",
            "    Uninstalling torch-2.3.1+cu121:\n",
            "      Successfully uninstalled torch-2.3.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-stubs 2.1.4.231227 requires numpy>=1.26.0; python_version < \"3.13\", but you have numpy 1.24.4 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.4 which is incompatible.\n",
            "torchaudio 2.3.1+cu121 requires torch==2.3.1, but you have torch 2.1.2 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.18.1+cu121 requires torch==2.3.1, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed einops-0.7.0 numpy-1.24.4 nvidia-nccl-cu12-2.18.1 protobuf-4.25.4 torch-2.1.2 triton-2.1.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "4e24cadf10ef497e97c804ba5ccb6c3c",
              "pip_warning": {
                "packages": [
                  "numpy",
                  "torch",
                  "torchgen"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!cd llama.cpp && make clean && make\n",
        "!pip install -r llama.cpp/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68564728",
      "metadata": {
        "id": "68564728"
      },
      "source": [
        "## Quantized GGUF Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6481b24c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6481b24c",
        "outputId": "48cf8a9c-4277-4088-a524-99ccf92cd301"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "main: build = 2760 (3f16747)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing './model/ggml-model-f16.gguf' to './model/ggml-model-q8_0.gguf' as Q8_0\n",
            "llama_model_loader: loaded meta data with 17 key-value pairs and 437 tensors from ./model/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gpt2\n",
            "llama_model_loader: - kv   1:                               general.name str              = model\n",
            "llama_model_loader: - kv   2:                           gpt2.block_count u32              = 36\n",
            "llama_model_loader: - kv   3:                        gpt2.context_length u32              = 1024\n",
            "llama_model_loader: - kv   4:                      gpt2.embedding_length u32              = 1280\n",
            "llama_model_loader: - kv   5:                   gpt2.feed_forward_length u32              = 5120\n",
            "llama_model_loader: - kv   6:                  gpt2.attention.head_count u32              = 20\n",
            "llama_model_loader: - kv   7:          gpt2.attention.layer_norm_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv   8:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv   9:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  10:                      tokenizer.ggml.tokens arr[str,50260]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  11:                  tokenizer.ggml.token_type arr[i32,50260]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.merges arr[str,50000]   = [\"Ġ t\", \"Ġ a\", \"h e\", \"i n\", \"r e\",...\n",
            "llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 50256\n",
            "llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 50256\n",
            "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 50256\n",
            "llama_model_loader: - kv  16:            tokenizer.ggml.padding_token_id u32              = 50257\n",
            "llama_model_loader: - type  f32:  290 tensors\n",
            "llama_model_loader: - type  f16:  147 tensors\n",
            "[   1/ 437]                  blk.0.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[   2/ 437]                blk.0.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[   3/ 437]               blk.0.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[   4/ 437]             blk.0.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[   5/ 437]                 blk.0.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[   6/ 437]               blk.0.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[   7/ 437]                  blk.0.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[   8/ 437]                blk.0.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[   9/ 437]                    blk.0.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  10/ 437]                  blk.0.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[  11/ 437]                  blk.0.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  12/ 437]                blk.0.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[  13/ 437]                  blk.1.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[  14/ 437]                blk.1.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[  15/ 437]               blk.1.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  16/ 437]             blk.1.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[  17/ 437]                 blk.1.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  18/ 437]               blk.1.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  19/ 437]                  blk.1.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  20/ 437]                blk.1.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  21/ 437]                    blk.1.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  22/ 437]                  blk.1.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[  23/ 437]                  blk.1.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  24/ 437]                blk.1.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[  25/ 437]                 blk.10.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[  26/ 437]               blk.10.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[  27/ 437]              blk.10.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  28/ 437]            blk.10.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[  29/ 437]                blk.10.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  30/ 437]              blk.10.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  31/ 437]                 blk.10.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  32/ 437]               blk.10.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  33/ 437]                   blk.10.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  34/ 437]                 blk.10.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[  35/ 437]                 blk.10.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  36/ 437]               blk.10.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[  37/ 437]                 blk.11.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[  38/ 437]               blk.11.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[  39/ 437]              blk.11.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  40/ 437]            blk.11.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[  41/ 437]                blk.11.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  42/ 437]              blk.11.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  43/ 437]                 blk.11.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  44/ 437]               blk.11.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  45/ 437]                   blk.11.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  46/ 437]                 blk.11.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[  47/ 437]                 blk.11.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  48/ 437]               blk.11.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[  49/ 437]                 blk.12.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[  50/ 437]               blk.12.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[  51/ 437]              blk.12.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  52/ 437]            blk.12.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[  53/ 437]                blk.12.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  54/ 437]              blk.12.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  55/ 437]                 blk.12.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  56/ 437]               blk.12.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  57/ 437]                   blk.12.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  58/ 437]                 blk.12.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[  59/ 437]                 blk.12.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  60/ 437]               blk.12.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[  61/ 437]                 blk.13.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[  62/ 437]               blk.13.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[  63/ 437]              blk.13.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  64/ 437]            blk.13.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[  65/ 437]                blk.13.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  66/ 437]              blk.13.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  67/ 437]                 blk.13.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  68/ 437]               blk.13.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  69/ 437]                   blk.13.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  70/ 437]                 blk.13.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[  71/ 437]                 blk.13.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  72/ 437]               blk.13.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[  73/ 437]                 blk.14.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[  74/ 437]               blk.14.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[  75/ 437]              blk.14.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  76/ 437]            blk.14.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[  77/ 437]                blk.14.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  78/ 437]              blk.14.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  79/ 437]                 blk.14.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  80/ 437]               blk.14.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  81/ 437]                   blk.14.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  82/ 437]                 blk.14.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[  83/ 437]                 blk.14.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  84/ 437]               blk.14.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[  85/ 437]                 blk.15.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[  86/ 437]               blk.15.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[  87/ 437]              blk.15.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  88/ 437]            blk.15.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[  89/ 437]                blk.15.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  90/ 437]              blk.15.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  91/ 437]                 blk.15.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  92/ 437]               blk.15.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  93/ 437]                   blk.15.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  94/ 437]                 blk.15.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[  95/ 437]                 blk.15.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[  96/ 437]               blk.15.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[  97/ 437]                 blk.16.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[  98/ 437]               blk.16.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[  99/ 437]              blk.16.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 100/ 437]            blk.16.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 101/ 437]                blk.16.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 102/ 437]              blk.16.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 103/ 437]                 blk.16.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 104/ 437]               blk.16.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 105/ 437]                   blk.16.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 106/ 437]                 blk.16.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 107/ 437]                 blk.16.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 108/ 437]               blk.16.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 109/ 437]                 blk.17.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 110/ 437]               blk.17.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 111/ 437]              blk.17.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 112/ 437]            blk.17.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 113/ 437]                blk.17.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 114/ 437]              blk.17.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 115/ 437]                 blk.17.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 116/ 437]               blk.17.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 117/ 437]                   blk.17.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 118/ 437]                 blk.17.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 119/ 437]                 blk.17.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 120/ 437]               blk.17.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 121/ 437]                 blk.18.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 122/ 437]               blk.18.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 123/ 437]              blk.18.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 124/ 437]            blk.18.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 125/ 437]                blk.18.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 126/ 437]              blk.18.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 127/ 437]                 blk.18.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 128/ 437]               blk.18.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 129/ 437]                   blk.18.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 130/ 437]                 blk.18.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 131/ 437]                 blk.18.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 132/ 437]               blk.18.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 133/ 437]                 blk.19.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 134/ 437]               blk.19.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 135/ 437]              blk.19.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 136/ 437]            blk.19.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 137/ 437]                blk.19.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 138/ 437]              blk.19.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 139/ 437]                 blk.19.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 140/ 437]               blk.19.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 141/ 437]                   blk.19.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 142/ 437]                 blk.19.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 143/ 437]                 blk.19.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 144/ 437]               blk.19.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 145/ 437]                  blk.2.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 146/ 437]                blk.2.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 147/ 437]               blk.2.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 148/ 437]             blk.2.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 149/ 437]                 blk.2.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 150/ 437]               blk.2.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 151/ 437]                  blk.2.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 152/ 437]                blk.2.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 153/ 437]                    blk.2.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 154/ 437]                  blk.2.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 155/ 437]                  blk.2.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 156/ 437]                blk.2.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 157/ 437]                 blk.20.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 158/ 437]               blk.20.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 159/ 437]              blk.20.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 160/ 437]            blk.20.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 161/ 437]                blk.20.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 162/ 437]              blk.20.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 163/ 437]                 blk.20.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 164/ 437]               blk.20.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 165/ 437]                   blk.20.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 166/ 437]                 blk.20.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 167/ 437]                 blk.20.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 168/ 437]               blk.20.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 169/ 437]                 blk.21.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 170/ 437]               blk.21.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 171/ 437]              blk.21.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 172/ 437]            blk.21.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 173/ 437]                blk.21.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 174/ 437]              blk.21.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 175/ 437]                 blk.21.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 176/ 437]               blk.21.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 177/ 437]                   blk.21.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 178/ 437]                 blk.21.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 179/ 437]                 blk.21.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 180/ 437]               blk.21.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 181/ 437]                 blk.22.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 182/ 437]               blk.22.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 183/ 437]              blk.22.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 184/ 437]            blk.22.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 185/ 437]                blk.22.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 186/ 437]              blk.22.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 187/ 437]                 blk.22.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 188/ 437]               blk.22.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 189/ 437]                   blk.22.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 190/ 437]                 blk.22.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 191/ 437]                 blk.22.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 192/ 437]               blk.22.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 193/ 437]                 blk.23.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 194/ 437]               blk.23.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 195/ 437]              blk.23.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 196/ 437]            blk.23.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 197/ 437]                blk.23.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 198/ 437]              blk.23.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 199/ 437]                 blk.23.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 200/ 437]               blk.23.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 201/ 437]                   blk.23.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 202/ 437]                 blk.23.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 203/ 437]                 blk.23.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 204/ 437]               blk.23.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 205/ 437]                 blk.24.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 206/ 437]               blk.24.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 207/ 437]              blk.24.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 208/ 437]            blk.24.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 209/ 437]                blk.24.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 210/ 437]              blk.24.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 211/ 437]                 blk.24.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 212/ 437]               blk.24.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 213/ 437]                   blk.24.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 214/ 437]                 blk.24.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 215/ 437]                 blk.24.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 216/ 437]               blk.24.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 217/ 437]                 blk.25.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 218/ 437]               blk.25.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 219/ 437]              blk.25.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 220/ 437]            blk.25.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 221/ 437]                blk.25.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 222/ 437]              blk.25.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 223/ 437]                 blk.25.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 224/ 437]               blk.25.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 225/ 437]                   blk.25.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 226/ 437]                 blk.25.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 227/ 437]                 blk.25.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 228/ 437]               blk.25.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 229/ 437]                 blk.26.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 230/ 437]               blk.26.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 231/ 437]              blk.26.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 232/ 437]            blk.26.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 233/ 437]                blk.26.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 234/ 437]              blk.26.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 235/ 437]                 blk.26.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 236/ 437]               blk.26.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 237/ 437]                   blk.26.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 238/ 437]                 blk.26.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 239/ 437]                 blk.26.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 240/ 437]               blk.26.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 241/ 437]                 blk.27.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 242/ 437]               blk.27.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 243/ 437]              blk.27.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 244/ 437]            blk.27.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 245/ 437]                blk.27.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 246/ 437]              blk.27.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 247/ 437]                 blk.27.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 248/ 437]               blk.27.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 249/ 437]                   blk.27.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 250/ 437]                 blk.27.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 251/ 437]                 blk.27.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 252/ 437]               blk.27.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 253/ 437]                 blk.28.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 254/ 437]               blk.28.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 255/ 437]              blk.28.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 256/ 437]            blk.28.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 257/ 437]                blk.28.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 258/ 437]              blk.28.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 259/ 437]                 blk.28.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 260/ 437]               blk.28.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 261/ 437]                   blk.28.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 262/ 437]                 blk.28.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 263/ 437]                 blk.28.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 264/ 437]               blk.28.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 265/ 437]                 blk.29.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 266/ 437]               blk.29.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 267/ 437]              blk.29.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 268/ 437]            blk.29.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 269/ 437]                blk.29.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 270/ 437]              blk.29.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 271/ 437]                 blk.29.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 272/ 437]               blk.29.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 273/ 437]                   blk.29.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 274/ 437]                 blk.29.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 275/ 437]                 blk.29.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 276/ 437]               blk.29.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 277/ 437]                  blk.3.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 278/ 437]                blk.3.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 279/ 437]               blk.3.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 280/ 437]             blk.3.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 281/ 437]                 blk.3.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 282/ 437]               blk.3.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 283/ 437]                  blk.3.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 284/ 437]                blk.3.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 285/ 437]                    blk.3.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 286/ 437]                  blk.3.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 287/ 437]                  blk.3.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 288/ 437]                blk.3.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 289/ 437]                 blk.30.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 290/ 437]               blk.30.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 291/ 437]              blk.30.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 292/ 437]            blk.30.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 293/ 437]                blk.30.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 294/ 437]              blk.30.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 295/ 437]                 blk.30.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 296/ 437]               blk.30.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 297/ 437]                   blk.30.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 298/ 437]                 blk.30.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 299/ 437]                 blk.30.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 300/ 437]               blk.30.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 301/ 437]                 blk.31.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 302/ 437]               blk.31.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 303/ 437]              blk.31.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 304/ 437]            blk.31.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 305/ 437]                blk.31.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 306/ 437]              blk.31.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 307/ 437]                 blk.31.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 308/ 437]               blk.31.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 309/ 437]                   blk.31.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 310/ 437]                 blk.31.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 311/ 437]                 blk.31.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 312/ 437]               blk.31.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 313/ 437]                 blk.32.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 314/ 437]               blk.32.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 315/ 437]              blk.32.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 316/ 437]            blk.32.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 317/ 437]                blk.32.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 318/ 437]              blk.32.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 319/ 437]                 blk.32.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 320/ 437]               blk.32.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 321/ 437]                   blk.32.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 322/ 437]                 blk.32.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 323/ 437]                 blk.32.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 324/ 437]               blk.32.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 325/ 437]                 blk.33.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 326/ 437]               blk.33.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 327/ 437]              blk.33.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 328/ 437]            blk.33.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 329/ 437]                blk.33.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 330/ 437]              blk.33.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 331/ 437]                 blk.33.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 332/ 437]               blk.33.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 333/ 437]                   blk.33.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 334/ 437]                 blk.33.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 335/ 437]                 blk.33.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 336/ 437]               blk.33.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 337/ 437]                 blk.34.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 338/ 437]               blk.34.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 339/ 437]              blk.34.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 340/ 437]            blk.34.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 341/ 437]                blk.34.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 342/ 437]              blk.34.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 343/ 437]                 blk.34.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 344/ 437]               blk.34.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 345/ 437]                   blk.34.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 346/ 437]                 blk.34.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 347/ 437]                 blk.34.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 348/ 437]               blk.34.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 349/ 437]                 blk.35.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 350/ 437]               blk.35.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 351/ 437]              blk.35.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 352/ 437]            blk.35.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 353/ 437]                blk.35.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 354/ 437]              blk.35.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 355/ 437]                 blk.35.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 356/ 437]               blk.35.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 357/ 437]                   blk.35.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 358/ 437]                 blk.35.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 359/ 437]                 blk.35.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 360/ 437]               blk.35.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 361/ 437]                  blk.4.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 362/ 437]                blk.4.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 363/ 437]               blk.4.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 364/ 437]             blk.4.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 365/ 437]                 blk.4.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 366/ 437]               blk.4.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 367/ 437]                  blk.4.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 368/ 437]                blk.4.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 369/ 437]                    blk.4.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 370/ 437]                  blk.4.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 371/ 437]                  blk.4.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 372/ 437]                blk.4.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 373/ 437]                  blk.5.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 374/ 437]                blk.5.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 375/ 437]               blk.5.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 376/ 437]             blk.5.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 377/ 437]                 blk.5.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 378/ 437]               blk.5.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 379/ 437]                  blk.5.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 380/ 437]                blk.5.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 381/ 437]                    blk.5.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 382/ 437]                  blk.5.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 383/ 437]                  blk.5.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 384/ 437]                blk.5.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 385/ 437]                  blk.6.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 386/ 437]                blk.6.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 387/ 437]               blk.6.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 388/ 437]             blk.6.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 389/ 437]                 blk.6.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 390/ 437]               blk.6.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 391/ 437]                  blk.6.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 392/ 437]                blk.6.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 393/ 437]                    blk.6.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 394/ 437]                  blk.6.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 395/ 437]                  blk.6.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 396/ 437]                blk.6.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 397/ 437]                  blk.7.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 398/ 437]                blk.7.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 399/ 437]               blk.7.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 400/ 437]             blk.7.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 401/ 437]                 blk.7.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 402/ 437]               blk.7.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 403/ 437]                  blk.7.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 404/ 437]                blk.7.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 405/ 437]                    blk.7.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 406/ 437]                  blk.7.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 407/ 437]                  blk.7.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 408/ 437]                blk.7.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 409/ 437]                  blk.8.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 410/ 437]                blk.8.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 411/ 437]               blk.8.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 412/ 437]             blk.8.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 413/ 437]                 blk.8.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 414/ 437]               blk.8.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 415/ 437]                  blk.8.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 416/ 437]                blk.8.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 417/ 437]                    blk.8.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 418/ 437]                  blk.8.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 419/ 437]                  blk.8.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 420/ 437]                blk.8.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 421/ 437]                  blk.9.attn_qkv.bias - [ 3840,     1,     1,     1], type =    f32, size =    0.015 MB\n",
            "[ 422/ 437]                blk.9.attn_qkv.weight - [ 1280,  3840,     1,     1], type =    f16, converting to q8_0 .. size =     9.38 MiB ->     4.98 MiB\n",
            "[ 423/ 437]               blk.9.attn_output.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 424/ 437]             blk.9.attn_output.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q8_0 .. size =     3.12 MiB ->     1.66 MiB\n",
            "[ 425/ 437]                 blk.9.attn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 426/ 437]               blk.9.attn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 427/ 437]                  blk.9.ffn_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 428/ 437]                blk.9.ffn_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 429/ 437]                    blk.9.ffn_up.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 430/ 437]                  blk.9.ffn_up.weight - [ 1280,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 431/ 437]                  blk.9.ffn_down.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 432/ 437]                blk.9.ffn_down.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q8_0 .. size =    12.50 MiB ->     6.64 MiB\n",
            "[ 433/ 437]                     output_norm.bias - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 434/ 437]                   output_norm.weight - [ 1280,     1,     1,     1], type =    f32, size =    0.005 MB\n",
            "[ 435/ 437]                 position_embd.weight - [ 1280,  1024,     1,     1], type =    f16, size =    2.500 MB\n",
            "[ 436/ 437]                    token_embd.weight - [ 1280, 50260,     1,     1], type =    f16, converting to q8_0 .. size =   122.71 MiB ->    65.19 MiB\n",
            "[ 437/ 437]                        output.weight - [ 1280, 50260,     1,     1], type =    f16, converting to q8_0 .. size =   122.71 MiB ->    65.19 MiB\n",
            "llama_model_quantize_internal: model size  =  1600.21 MB\n",
            "llama_model_quantize_internal: quant size  =   852.36 MB\n",
            "\n",
            "main: quantize time = 18350.60 ms\n",
            "main:    total time = 18350.60 ms\n"
          ]
        }
      ],
      "source": [
        "!./llama.cpp/quantize ./model/ggml-model-f16.gguf ./model/ggml-model-q8_0.gguf q8_0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c2fb286",
      "metadata": {
        "id": "8c2fb286"
      },
      "source": [
        "## Install Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33ab28a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33ab28a8",
        "outputId": "41e10f90-c8fc-49ee-e6e6-7d7885c9fc05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0>>> Downloading ollama...\n",
            "100 11868    0 11868    0     0  28989      0 --:--:-- --:--:-- --:--:-- 29017\n",
            "############################################################################################# 100.0%\n",
            ">>> Installing ollama to /usr/local/bin...\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Collecting ollama\n",
            "  Downloading ollama-0.3.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting httpx<0.28.0,>=0.27.0 (from ollama)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<0.28.0,>=0.27.0->ollama)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (1.2.2)\n",
            "Downloading ollama-0.3.1-py3-none-any.whl (10 kB)\n",
            "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, ollama\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 ollama-0.3.1\n"
          ]
        }
      ],
      "source": [
        "!curl https://ollama.ai/install.sh | sh\n",
        "!pip install ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2450ab8d",
      "metadata": {
        "id": "2450ab8d"
      },
      "source": [
        "## Helper Functions for Serving Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cd3ad19",
      "metadata": {
        "id": "0cd3ad19"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/77697302/how-to-run-ollama-in-google-colab\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "\n",
        "# NB: You may need to set these depending and get cuda working depending which backend you are running.\n",
        "# Set environment variable for NVIDIA library\n",
        "# Set environment variables for CUDA\n",
        "os.environ['PATH'] += ':/usr/local/cuda/bin'\n",
        "# Set LD_LIBRARY_PATH to include both /usr/lib64-nvidia and CUDA lib directories\n",
        "os.environ['LD_LIBRARY_PATH'] = '/usr/lib64-nvidia:/usr/local/cuda/lib64'\n",
        "\n",
        "async def run_process(cmd):\n",
        "    print('>>> starting', *cmd)\n",
        "    process = await asyncio.create_subprocess_exec(\n",
        "        *cmd,\n",
        "        stdout=asyncio.subprocess.PIPE,\n",
        "        stderr=asyncio.subprocess.PIPE\n",
        "    )\n",
        "\n",
        "    # define an async pipe function\n",
        "    async def pipe(lines):\n",
        "        async for line in lines:\n",
        "            print(line.decode().strip())\n",
        "\n",
        "        await asyncio.gather(\n",
        "            pipe(process.stdout),\n",
        "            pipe(process.stderr),\n",
        "        )\n",
        "\n",
        "    # call it\n",
        "    await asyncio.gather(pipe(process.stdout), pipe(process.stderr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d7d98cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d7d98cf",
        "outputId": "1432a6bd-b207-46ac-bb18-44084b083fae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> starting ollama serve\n",
            "2024/08/07 16:24:46 routes.go:1108: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]\"\n",
            "time=2024-08-07T16:24:46.408Z level=INFO source=images.go:781 msg=\"total blobs: 2\"\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import threading\n",
        "\n",
        "async def start_ollama_serve():\n",
        "    await run_process(['ollama', 'serve'])\n",
        "\n",
        "def run_async_in_thread(loop, coro):\n",
        "    asyncio.set_event_loop(loop)\n",
        "    loop.run_until_complete(coro)\n",
        "    loop.close()\n",
        "\n",
        "# Create a new event loop that will run in a new thread\n",
        "new_loop = asyncio.new_event_loop()\n",
        "\n",
        "# Start ollama serve in a separate thread so the cell won't block execution\n",
        "thread = threading.Thread(target=run_async_in_thread, args=(new_loop, start_ollama_serve()))\n",
        "thread.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56da8fb9",
      "metadata": {
        "id": "56da8fb9"
      },
      "source": [
        "## Custom Model File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50167caa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50167caa",
        "outputId": "b0a24d42-b2b0-41da-856a-92d11c61c4b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.chat_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uljGc6NYORvP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uljGc6NYORvP",
        "outputId": "7ee51871-423d-4802-d2c9-a84e8723a49f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['<|endoftext|>', '[PAD]', '<|USER|>', '<|ASSISTANT|>'],\n",
              " ['<|USER|>', '<|ASSISTANT|>'])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.all_special_tokens, tokenizer.additional_special_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xSRktHfpMOV-",
      "metadata": {
        "id": "xSRktHfpMOV-"
      },
      "outputs": [],
      "source": [
        "# https://huggingface.co/Locutusque/gpt2-large-conversational\n",
        "\n",
        "modelfile = ('FROM ./model/ggml-model-q8_0.gguf'\n",
        "'\\n\\nTEMPLATE \"\"\"'\n",
        "'\\n<|USER|> {{ .Prompt }} <|ASSISTANT|> {{ .Response }} <|endoftext|>'\n",
        "'\\n\"\"\"'\n",
        "'\\n\\nPARAMETER stop <|endoftext|>'\n",
        "'\\nPARAMETER stop [PAD]'\n",
        "'\\nPARAMETER stop <|ASSISTANT|>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XO6F3oSjejBR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO6F3oSjejBR",
        "outputId": "41e0159a-b880-4a28-cd59-7f296c58d5bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FROM ./model/ggml-model-q8_0.gguf\n",
            "\n",
            "TEMPLATE \"\"\"\n",
            "<|USER|> {{ .Prompt }} <|ASSISTANT|> {{ .Response }} <|endoftext|>\n",
            "\"\"\"\n",
            "\n",
            "PARAMETER stop <|endoftext|>\n",
            "PARAMETER stop [PAD]\n",
            "PARAMETER stop <|ASSISTANT|>\n"
          ]
        }
      ],
      "source": [
        "print(modelfile)\n",
        "\n",
        "with open('modelfile_custom', 'w') as f:\n",
        "    f.write(modelfile)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6092ade",
      "metadata": {
        "id": "c6092ade"
      },
      "source": [
        "## Loading Model into Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "003e784f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "003e784f",
        "outputId": "d5672653-9378-499c-a92f-9e9da1b52006"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[GIN] 2024/08/07 - 16:55:01 | 200 |      32.516µs |       127.0.0.1 | HEAD     \"/\"\n",
            "\u001b[?25ltransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h[GIN] 2024/08/07 - 16:55:06 | 200 |        99.7µs |       127.0.0.1 | POST     \"/api/blobs/sha256:d7c8d15dd801dd846d8272f153cd64d9f93657dbc85152bea48b40c8c9b5b382\"\n",
            "\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h[GIN] 2024/08/07 - 16:55:18 | 200 | 12.635422931s |       127.0.0.1 | POST     \"/api/create\"\n",
            "\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data \n",
            "using existing layer sha256:d7c8d15dd801dd846d8272f153cd64d9f93657dbc85152bea48b40c8c9b5b382 \n",
            "using existing layer sha256:1a655ad922c176809d33c3c058348a1118ea6026bf03a5ef33087590aed8bc57 \n",
            "creating new layer sha256:093f717c00c626a5dde20fea025623540685f8ebcb28c8658789a60c493e42fb \n",
            "creating new layer sha256:5cc91be23945d6a6b9f393eca740d8ff2d4bcffec870421f02fbfd5ec87a4333 \n",
            "writing manifest \n",
            "success \u001b[?25h\n"
          ]
        }
      ],
      "source": [
        "!ollama create -f modelfile_custom medical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uenvQJYMe0h2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uenvQJYMe0h2",
        "outputId": "eb047d89-ee24-44e0-96cf-c54c3bb9ad7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[GIN] 2024/08/07 - 16:56:03 | 200 |       32.73µs |       127.0.0.1 | HEAD     \"/\"\n",
            "[GIN] 2024/08/07 - 16:56:03 | 200 |     540.998µs |       127.0.0.1 | GET      \"/api/tags\"\n",
            "NAME          \tID          \tSIZE  \tMODIFIED       \n",
            "medical:latest\tf3161abed622\t895 MB\t44 seconds ago\t\n"
          ]
        }
      ],
      "source": [
        "!ollama list"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea048192",
      "metadata": {
        "id": "ea048192"
      },
      "source": [
        "## Querying the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TRorNdQPr6Cu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRorNdQPr6Cu",
        "outputId": "5e8c2cd5-d407-4fb2-86d1-7509e88d1646"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[GIN] 2024/08/07 - 16:59:25 | 200 |  776.614301ms |       127.0.0.1 | POST     \"/api/chat\"\n"
          ]
        }
      ],
      "source": [
        "import ollama\n",
        "response = ollama.chat(model='medical', messages=[\n",
        "  {\n",
        "    'role': 'user',\n",
        "    'content': 'I got a bad headache on the left side of my forehead. How can I get better?',\n",
        "  },\n",
        "], stream=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MzbB9mATvn8c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzbB9mATvn8c",
        "outputId": "944a20cc-342b-45bc-bfc2-a4650c860469"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " an anti-inflammatory medication can be prescribed to relieve pain caused by inflammation of the brain and the blood vessels in the brain. This medication is called aspirin and it is used to treat headaches. It should not be used if you have already had a stroke or if there is any bleeding in your brain.\n"
          ]
        }
      ],
      "source": [
        "print(response['message']['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "665jOth-xrrg",
      "metadata": {
        "id": "665jOth-xrrg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00875a803ab9495c97de53b5329a66d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cd7c64331c14e39b31588a22d16967e",
            "placeholder": "​",
            "style": "IPY_MODEL_43dda54f43a44f5398406e9b3806c5de",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "01484c611d2d484bb2f8fc5f10b4bd3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "048f0ba88f72453fb6e36659674e2c91": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04959f12e8e84724859efc0a119a3a92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb97abf996bc4c7d80d77e1069b013b0",
            "placeholder": "​",
            "style": "IPY_MODEL_fbd85da9eb32470c9218e78fe4c5743b",
            "value": "vocab.json: 100%"
          }
        },
        "04d7c2674bfe4632b4e10e57f9f6af69": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "061fa59d25c64a9584b3e231fd8e91f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5d8809d1bee48a684814e46b776a8f4",
            "placeholder": "​",
            "style": "IPY_MODEL_4f377e89dffa4abbba1ca2db0d746cc9",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "0b048bf56c96483188eb20c1e41e0f67": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b69df942672445bae73504390278833": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0cd7c64331c14e39b31588a22d16967e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f427ce898df4dd7a024ab714bb72eac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb7d7291e67b4f33ba76b6a4b1d27e68",
            "placeholder": "​",
            "style": "IPY_MODEL_27a244515f7f4cce965e5c2695faede3",
            "value": "tokenizer.json: 100%"
          }
        },
        "0f5093a96ac340ce98c1967cf8c65c70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14d5f9af3fb44e18ac3d4e7e22059d9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f427ce898df4dd7a024ab714bb72eac",
              "IPY_MODEL_924d52e3548647bcad110214542bbfda",
              "IPY_MODEL_484efcf7ca924b688c9e98ae109ebfc1"
            ],
            "layout": "IPY_MODEL_2cc4c9ead9fc45f8b2c00c3268de0bf3"
          }
        },
        "1631c1159cda4d6f8e1f68f847aa6f3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "17bffe95e5d6473cba7ef628144c8a83": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1abd33e170f8487b89beadbefc5ecc84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_755130fa95c344b296cb957115f60c05",
            "placeholder": "​",
            "style": "IPY_MODEL_7dccb1bf733346ad90931f40d15ea6b8",
            "value": " 915/915 [00:00&lt;00:00, 29.8kB/s]"
          }
        },
        "1d64f21e21df4796b4e0bbef9a8db93d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "213c83a374f54b0b9530f4e7e74c517c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0f8669799044a558155ac8cd28eab6b",
              "IPY_MODEL_bc03cc02710e4c4eb1092884cc185648",
              "IPY_MODEL_8ffd8b7eb3de41f0931d32aae0f9ad4a"
            ],
            "layout": "IPY_MODEL_17bffe95e5d6473cba7ef628144c8a83"
          }
        },
        "24058594877f4d4884c5b714f78dc88f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25242e51996c4235ba92fac88eff0e50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27a244515f7f4cce965e5c2695faede3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c5a77bb4bc4464b8c2bdb30d09d575e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8799e4e6a25484ab02130bff9615692",
            "placeholder": "​",
            "style": "IPY_MODEL_cb67fe88c1144e0f9dc2c8755d284667",
            "value": "added_tokens.json: 100%"
          }
        },
        "2cc4c9ead9fc45f8b2c00c3268de0bf3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ee3d2dacf814df3b9ff55d4f1d78e48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3101fcb47d204ea5b76972e194fa6a00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_00875a803ab9495c97de53b5329a66d3",
              "IPY_MODEL_61c013cf59ca4729b30d37a3959a4847",
              "IPY_MODEL_f990c9a5128d49b69cb706f4a3e25f61"
            ],
            "layout": "IPY_MODEL_debf95056f8b44d5b3ff0256982f07a9"
          }
        },
        "3427805c48cf465f8e57a8953d37c078": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37db8232247d4620a15737be1e47217c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c5fc4681099425b8ea3577758f9a940",
            "placeholder": "​",
            "style": "IPY_MODEL_0f5093a96ac340ce98c1967cf8c65c70",
            "value": " 234/234 [00:00&lt;00:00, 7.92kB/s]"
          }
        },
        "3a4a183a5cc2445c969d1e083896bb7e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b04a0ea7c454c6b8dd473137bd495a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c63eb1d7f8140c984b1c0554b769fe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43dda54f43a44f5398406e9b3806c5de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4601955d3c10426f9b80244feefb22b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46e4143958c24c1c9be5fb703207a8ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "484efcf7ca924b688c9e98ae109ebfc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74fd404e3b6a4a0191bb405ec814d0d8",
            "placeholder": "​",
            "style": "IPY_MODEL_4c93dfd6c8024ff8a0e5c1f5b3fef58b",
            "value": " 2.11M/2.11M [00:00&lt;00:00, 9.79MB/s]"
          }
        },
        "48c5105429a140f3bcb676a5be27728f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "495b4930606248cfb82694c15df9e689": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b048bf56c96483188eb20c1e41e0f67",
            "placeholder": "​",
            "style": "IPY_MODEL_3c63eb1d7f8140c984b1c0554b769fe2",
            "value": "generation_config.json: 100%"
          }
        },
        "4c93dfd6c8024ff8a0e5c1f5b3fef58b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f377e89dffa4abbba1ca2db0d746cc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54b7f4ee7e85411c88985ea6b6c051c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "550c9fab7d6346c5a2fab6a1d86609ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5873674d95446a0817329aaadd0c771",
            "placeholder": "​",
            "style": "IPY_MODEL_b060011427fe40f3bfcd9565d8879e10",
            "value": " 798k/798k [00:00&lt;00:00, 3.72MB/s]"
          }
        },
        "552044de39e14a22bbb27b39bb9a337d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c67ba8bdfdf24acdace130683152c38e",
              "IPY_MODEL_606984fe981c462f96d0aca1e52f280a",
              "IPY_MODEL_1abd33e170f8487b89beadbefc5ecc84"
            ],
            "layout": "IPY_MODEL_e401fc2cbc7a4ac2b7a5d54b8148ceb6"
          }
        },
        "55e2eba5b5b84cd1a7b6453733feef90": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c5fc4681099425b8ea3577758f9a940": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d02693fea674a42b92f5455f591b54c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "606984fe981c462f96d0aca1e52f280a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a664f9dbdce74ef2a1a91fd71a60a5ad",
            "max": 915,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1631c1159cda4d6f8e1f68f847aa6f3f",
            "value": 915
          }
        },
        "61c013cf59ca4729b30d37a3959a4847": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86f5539647d845258780e6629ef4aefd",
            "max": 197,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_64e9791479be40668aaad4d31b7f6f39",
            "value": 197
          }
        },
        "64e9791479be40668aaad4d31b7f6f39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "661d989d3d554317a232ebe98f73454f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1a06b050fe649f28088176ffb721119",
            "max": 234,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f66502075a9447f88ca8843817e3380b",
            "value": 234
          }
        },
        "6b42acb4c79042fe95e77fa03a28ac6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6d76d349991c46ceafc3523c09ec8be0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74fd404e3b6a4a0191bb405ec814d0d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "755130fa95c344b296cb957115f60c05": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76b83c075e2440cd80a9eedd452d2b63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7768e51462ee42c2b54ce4d5d362756c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54b7f4ee7e85411c88985ea6b6c051c2",
            "max": 68,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_46e4143958c24c1c9be5fb703207a8ad",
            "value": 68
          }
        },
        "7dccb1bf733346ad90931f40d15ea6b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f650bb4284e475fb633094f3000114c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86f5539647d845258780e6629ef4aefd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89021f1f1c8846648698b25941ccca25": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ad11d7a8a934456a0409a10211529b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cd90f2249f6450ca9d76f5435f2ac8d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ffd8b7eb3de41f0931d32aae0f9ad4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cd90f2249f6450ca9d76f5435f2ac8d",
            "placeholder": "​",
            "style": "IPY_MODEL_3427805c48cf465f8e57a8953d37c078",
            "value": " 1.55G/1.55G [01:55&lt;00:00, 15.1MB/s]"
          }
        },
        "924d52e3548647bcad110214542bbfda": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48c5105429a140f3bcb676a5be27728f",
            "max": 2108305,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0b69df942672445bae73504390278833",
            "value": 2108305
          }
        },
        "96ed0bf05fe945faa45155540d7d740f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98f5bc345bdd4e91b50b0ed5a67df1d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1a06b050fe649f28088176ffb721119": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5873674d95446a0817329aaadd0c771": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a664f9dbdce74ef2a1a91fd71a60a5ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6d76ca08ed04c418d7d7765e95bb026": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7c6b1fb808342749259e03ecbbfeba4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac730caae04041399041db579827a6aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b060011427fe40f3bfcd9565d8879e10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b27fd703ed994a4dad04ffa7204677ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_061fa59d25c64a9584b3e231fd8e91f9",
              "IPY_MODEL_661d989d3d554317a232ebe98f73454f",
              "IPY_MODEL_37db8232247d4620a15737be1e47217c"
            ],
            "layout": "IPY_MODEL_fec58d1bc38d42c5b4c5f02d2b36ec1c"
          }
        },
        "b8799e4e6a25484ab02130bff9615692": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb7d7291e67b4f33ba76b6a4b1d27e68": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc03cc02710e4c4eb1092884cc185648": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d76d349991c46ceafc3523c09ec8be0",
            "max": 1548113536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b42acb4c79042fe95e77fa03a28ac6b",
            "value": 1548113536
          }
        },
        "bc60b400404d44fe9b91af871d74f0b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55e2eba5b5b84cd1a7b6453733feef90",
            "max": 119,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d64f21e21df4796b4e0bbef9a8db93d",
            "value": 119
          }
        },
        "c0f8669799044a558155ac8cd28eab6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_048f0ba88f72453fb6e36659674e2c91",
            "placeholder": "​",
            "style": "IPY_MODEL_98f5bc345bdd4e91b50b0ed5a67df1d7",
            "value": "model.safetensors: 100%"
          }
        },
        "c1848ff32e88434a9d27bce93e008b30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca4cd11acb964e0aab3b939d86f7179d",
            "placeholder": "​",
            "style": "IPY_MODEL_01484c611d2d484bb2f8fc5f10b4bd3d",
            "value": "merges.txt: 100%"
          }
        },
        "c67ba8bdfdf24acdace130683152c38e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ee3d2dacf814df3b9ff55d4f1d78e48",
            "placeholder": "​",
            "style": "IPY_MODEL_4601955d3c10426f9b80244feefb22b4",
            "value": "config.json: 100%"
          }
        },
        "ca4cd11acb964e0aab3b939d86f7179d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb67fe88c1144e0f9dc2c8755d284667": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdbb605502614593a621b61251d3446d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c5a77bb4bc4464b8c2bdb30d09d575e",
              "IPY_MODEL_7768e51462ee42c2b54ce4d5d362756c",
              "IPY_MODEL_f849100d1d5c479ab96432b845bbcf14"
            ],
            "layout": "IPY_MODEL_04d7c2674bfe4632b4e10e57f9f6af69"
          }
        },
        "d04240f7ca5048829bb888d3de1e6240": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2bf83e6a0af48ac93b5ea258977d6b9",
            "max": 798156,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b04a0ea7c454c6b8dd473137bd495a0",
            "value": 798156
          }
        },
        "d5d8809d1bee48a684814e46b776a8f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6039c2a9f7a4f3abe58a50a4c8f11ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_04959f12e8e84724859efc0a119a3a92",
              "IPY_MODEL_d04240f7ca5048829bb888d3de1e6240",
              "IPY_MODEL_550c9fab7d6346c5a2fab6a1d86609ef"
            ],
            "layout": "IPY_MODEL_3a4a183a5cc2445c969d1e083896bb7e"
          }
        },
        "d9167d8be97b443fb655790037f51657": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c1848ff32e88434a9d27bce93e008b30",
              "IPY_MODEL_e11cba9a79ac48f290a14b58d82600de",
              "IPY_MODEL_e3b3b18623214890a0ec08e28d475c2c"
            ],
            "layout": "IPY_MODEL_a7c6b1fb808342749259e03ecbbfeba4"
          }
        },
        "dd86f0e7deb84d1dac7a7ca9bd3aa1d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_495b4930606248cfb82694c15df9e689",
              "IPY_MODEL_bc60b400404d44fe9b91af871d74f0b2",
              "IPY_MODEL_f15ccd34d13a4b0a922a0c8437a9befe"
            ],
            "layout": "IPY_MODEL_24058594877f4d4884c5b714f78dc88f"
          }
        },
        "debf95056f8b44d5b3ff0256982f07a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e11cba9a79ac48f290a14b58d82600de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e92203bcba244e17ab78f9ea7ad5937d",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac730caae04041399041db579827a6aa",
            "value": 456318
          }
        },
        "e3b3b18623214890a0ec08e28d475c2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89021f1f1c8846648698b25941ccca25",
            "placeholder": "​",
            "style": "IPY_MODEL_a6d76ca08ed04c418d7d7765e95bb026",
            "value": " 456k/456k [00:00&lt;00:00, 7.09MB/s]"
          }
        },
        "e401fc2cbc7a4ac2b7a5d54b8148ceb6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e92203bcba244e17ab78f9ea7ad5937d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f15ccd34d13a4b0a922a0c8437a9befe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f650bb4284e475fb633094f3000114c",
            "placeholder": "​",
            "style": "IPY_MODEL_25242e51996c4235ba92fac88eff0e50",
            "value": " 119/119 [00:00&lt;00:00, 7.56kB/s]"
          }
        },
        "f2bf83e6a0af48ac93b5ea258977d6b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f66502075a9447f88ca8843817e3380b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f849100d1d5c479ab96432b845bbcf14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96ed0bf05fe945faa45155540d7d740f",
            "placeholder": "​",
            "style": "IPY_MODEL_76b83c075e2440cd80a9eedd452d2b63",
            "value": " 68.0/68.0 [00:00&lt;00:00, 1.59kB/s]"
          }
        },
        "f990c9a5128d49b69cb706f4a3e25f61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ad11d7a8a934456a0409a10211529b1",
            "placeholder": "​",
            "style": "IPY_MODEL_5d02693fea674a42b92f5455f591b54c",
            "value": " 197/197 [00:00&lt;00:00, 3.10kB/s]"
          }
        },
        "fb97abf996bc4c7d80d77e1069b013b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbd85da9eb32470c9218e78fe4c5743b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fec58d1bc38d42c5b4c5f02d2b36ec1c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}