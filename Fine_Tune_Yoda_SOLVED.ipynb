{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NID123-CH/LLM-Codes/blob/main/Fine_Tune_Yoda_SOLVED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8b9b585",
      "metadata": {
        "scrolled": true,
        "id": "a8b9b585",
        "outputId": "71e097d9-d66f-4edc-85d6-c8f39ed08afa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting accelerate\n",
            "  Using cached accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting datasets\n",
            "  Using cached datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting peft\n",
            "  Using cached peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting trl\n",
            "  Using cached trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting bitsandbytes\n",
            "  Using cached bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting matplotlib\n",
            "  Using cached matplotlib-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/micromamba/envs/python_310/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/micromamba/envs/python_310/lib/python3.10/site-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /opt/micromamba/envs/python_310/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
            "Requirement already satisfied: pyyaml in /opt/micromamba/envs/python_310/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /opt/micromamba/envs/python_310/lib/python3.10/site-packages (from accelerate) (2.2.1)\n",
            "Collecting huggingface-hub (from accelerate)\n",
            "  Using cached huggingface_hub-0.23.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting safetensors>=0.3.1 (from accelerate)\n",
            "  Using cached safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: filelock in /opt/micromamba/envs/python_310/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
            "Collecting pyarrow>=12.0.0 (from datasets)\n",
            "  Using cached pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pandas (from datasets)\n",
            "  Using cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: requests>=2.19.0 in /opt/micromamba/envs/python_310/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
            "Collecting tqdm>=4.62.1 (from datasets)\n",
            "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.3.1,>=2023.1.0 (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets)\n",
            "  Using cached fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting aiohttp (from datasets)\n",
            "  Using cached aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting transformers (from peft)\n",
            "  Using cached transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
            "Collecting tyro>=0.5.11 (from trl)\n",
            "  Using cached tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Using cached contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Using cached fonttools-4.51.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (159 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Using cached kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: pillow>=8 in /opt/micromamba/envs/python_310/lib/python3.10/site-packages (from matplotlib) (8.4.0)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
            "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /opt/micromamba/envs/python_310/lib/python3.10/site-packages (from matplotlib) (2.9.0)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
            "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
            "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
            "  Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
            "  Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
            "  Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
            "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
            "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/micromamba/envs/python_310/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/micromamba/envs/python_310/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/micromamba/envs/python_310/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/micromamba/envs/python_310/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/micromamba/envs/python_310/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/micromamba/envs/python_310/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: sympy in /opt/micromamba/envs/python_310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /opt/micromamba/envs/python_310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/micromamba/envs/python_310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Collecting regex!=2019.12.17 (from transformers->peft)\n",
            "  Using cached regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers->peft)\n",
            "  Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl)\n",
            "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting rich>=11.1.0 (from tyro>=0.5.11->trl)\n",
            "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n",
            "  Using cached shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting pytz>=2020.1 (from pandas->datasets)\n",
            "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
            "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl)\n",
            "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/micromamba/envs/python_310/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.17.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/micromamba/envs/python_310/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/micromamba/envs/python_310/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl)\n",
            "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Using cached accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "Using cached datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "Using cached peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "Using cached trl-0.8.6-py3-none-any.whl (245 kB)\n",
            "Using cached bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "Using cached matplotlib-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "Using cached contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (305 kB)\n",
            "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Using cached fonttools-4.51.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "Using cached aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Using cached huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "Using cached kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "Using cached pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
            "Using cached safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "Using cached transformers-4.41.0-py3-none-any.whl (9.1 MB)\n",
            "Using cached tyro-0.8.4-py3-none-any.whl (102 kB)\n",
            "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "Using cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "Using cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
            "Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
            "Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
            "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
            "Using cached regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
            "Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
            "Using cached shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
            "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: pytz, xxhash, tzdata, tqdm, shtab, safetensors, regex, pyparsing, pyarrow-hotfix, pyarrow, multidict, mdurl, kiwisolver, fsspec, frozenlist, fonttools, docstring-parser, dill, cycler, contourpy, attrs, async-timeout, yarl, pandas, multiprocess, matplotlib, markdown-it-py, huggingface-hub, aiosignal, tokenizers, rich, bitsandbytes, aiohttp, accelerate, tyro, transformers, peft, datasets, trl\n",
            "Successfully installed accelerate-0.30.1 aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.2.0 bitsandbytes-0.43.1 contourpy-1.2.1 cycler-0.12.1 datasets-2.19.1 dill-0.3.8 docstring-parser-0.16 fonttools-4.51.0 frozenlist-1.4.1 fsspec-2024.3.1 huggingface-hub-0.23.0 kiwisolver-1.4.5 markdown-it-py-3.0.0 matplotlib-3.9.0 mdurl-0.1.2 multidict-6.0.5 multiprocess-0.70.16 pandas-2.2.2 peft-0.11.1 pyarrow-16.1.0 pyarrow-hotfix-0.6 pyparsing-3.1.2 pytz-2024.1 regex-2024.5.15 rich-13.7.1 safetensors-0.4.3 shtab-1.7.1 tokenizers-0.19.1 tqdm-4.66.4 transformers-4.41.0 trl-0.8.6 tyro-0.8.4 tzdata-2024.1 xxhash-3.4.1 yarl-1.9.4\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate datasets peft trl bitsandbytes matplotlib gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d2788d6",
      "metadata": {
        "id": "8d2788d6"
      },
      "source": [
        "# Exercise\n",
        "\n",
        "In this exercise, we'll fine-tune Phi-2 to translate sentences from English to the way Yoda talks.\n",
        "\n",
        "In order to accomplish that, we'll create a \"response template\", that is, a special token that triggers the translation. We'll use the token `##[YODA]##>` so, whenever it is added at the end of a sentence, the model should complete it with the translated version.\n",
        "\n",
        "For example, given the prompt:\n",
        "\n",
        "`There is bacon in the sandwich.##[YODA]##>`\n",
        "\n",
        "It should complete the sentence like this:\n",
        "\n",
        "`There is bacon in the sandwich.##[YODA]##>Bacon in the sandwich there is.`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8a9b1fc",
      "metadata": {
        "id": "c8a9b1fc"
      },
      "source": [
        "## Yoda\n",
        "\n",
        "Download the CSV file and load it using [`load_dataset()`](https://huggingface.co/docs/datasets/en/loading). Then, shuffle the dataset and split it into train and test sets ([preprocessing a dataset](https://huggingface.co/docs/datasets/en/process))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2d82781",
      "metadata": {
        "id": "b2d82781"
      },
      "outputs": [],
      "source": [
        "# Downloads yoda_translation.csv\n",
        "!gdown 1luZxKTMuV2E6IGoHI9UARdOFGYAOfBMy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8406a24",
      "metadata": {
        "id": "e8406a24"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Split\n",
        "dataset = load_dataset(path='csv', data_files='yoda_translation.csv', quotechar='\"', split=Split.ALL)\n",
        "dataset = dataset.shuffle().train_test_split(test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47e76944",
      "metadata": {
        "scrolled": true,
        "id": "47e76944",
        "outputId": "f56501e5-2d31-496e-b380-bfc909860d58"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence', 'yoda'],\n",
              "        num_rows: 576\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence', 'yoda'],\n",
              "        num_rows: 144\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec4bc1dc",
      "metadata": {
        "id": "ec4bc1dc"
      },
      "source": [
        "Take a look at one element of the training set. It should have two columns: `sentence` and `yoda` (the translated sentence)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b511fdd",
      "metadata": {
        "id": "5b511fdd",
        "outputId": "ea2c31a5-e4cd-4766-e417-634f78339d77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'sentence': 'No cement will hold hard wood.',\n",
              " 'yoda': 'Hard wood no cement will hold.'}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f2217ac",
      "metadata": {
        "id": "6f2217ac"
      },
      "source": [
        "### Prompt Dataset\n",
        "\n",
        "Now, let's make it a \"prompt dataset\" by renaming the columns to `prompt` and `completion` ([preprocessing a dataset](https://huggingface.co/docs/datasets/en/process)).\n",
        "\n",
        "We'll train the model to take a regular English sentence (the prompt) and produce an output (that is, complete the sentence) with the Yoda translation (completion)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "023c23e9",
      "metadata": {
        "id": "023c23e9"
      },
      "outputs": [],
      "source": [
        "prompt_yoda = dataset.rename_columns({'sentence': 'prompt', 'yoda': 'completion'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b8108d7",
      "metadata": {
        "id": "5b8108d7",
        "outputId": "c2f8d456-cd53-4117-8481-65efa79a9984"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'prompt': 'No cement will hold hard wood.',\n",
              " 'completion': 'Hard wood no cement will hold.'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_yoda['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbc9a7da",
      "metadata": {
        "id": "dbc9a7da"
      },
      "source": [
        "## Tokenizer\n",
        "\n",
        "Use HF's [`AutoTokenizer`](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer) to create a tokenizer for `microsoft/phi-2` model.\n",
        "\n",
        "The parameters for Phi-2 can be found [here](https://huggingface.co/docs/transformers/main/en/model_doc/codegen#transformers.CodeGenTokenizer). Make sure you add a begin of sentence (BOS) and a padding token (`<|pad|>`) as well.\n",
        "\n",
        "We'll need to pad it on the left side (cause we're generating new words starting on the end - the right side). You can force the tokenizer to pad on the left by using `padding_side=\"left\"`. Moreover, we have to set `use_fast=False` because Phi's tokenizer does not support the fast tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f5fcf9c",
      "metadata": {
        "id": "3f5fcf9c",
        "outputId": "caeb768a-d996-4de3-961d-752fcdac7d7e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "base_model_id = 'microsoft/phi-2'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    padding_side=\"left\",\n",
        "    add_bos_token=True,\n",
        "    use_fast=False,\n",
        "    pad_token='<|pad|>'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d509a2e",
      "metadata": {
        "id": "7d509a2e"
      },
      "source": [
        "Our \"Yoda\" token isn't any of the expected special tokens (padding, unknown, mask, etc.). It is an *additional special token*. Luckily, there is a method to add such tokens to the tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43f3c587-a584-47aa-a229-5cd4156bb9bb",
      "metadata": {
        "id": "43f3c587-a584-47aa-a229-5cd4156bb9bb",
        "outputId": "36b67c1a-1bb2-4742-feee-af4d2035d06e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50297"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response_template = '##[YODA]##>'\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': [response_template]})\n",
        "\n",
        "len(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88721eba-6779-4306-b21d-e6fbb0f70c60",
      "metadata": {
        "id": "88721eba-6779-4306-b21d-e6fbb0f70c60",
        "outputId": "ddb07edd-be68-4fc4-a4c1-c95664329087"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('<|pad|>', '<|endoftext|>')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.pad_token, tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b591b27",
      "metadata": {
        "id": "7b591b27"
      },
      "source": [
        "### Formatting\n",
        "\n",
        "Let's build a formatting function that takes both prompt and completion, and inserts a particular string that will be used to trigger the translation. This string is the response template (`##[YODA]##>`) as previously discussed.\n",
        "\n",
        "The formatting function should produce outputs such as this one:\n",
        "\n",
        "`There is bacon in the sandwich.##[YODA]##>Bacon in the sandwich there is.`\n",
        "\n",
        "However, there is one small - yet important - detail to add: we should add the EOS token to the end of the sentence in order to signal to the model that it should stop the generation at that point.\n",
        "\n",
        "So, the output should really look like this:\n",
        "\n",
        "`There is bacon in the sandwich.##[YODA]##>Bacon in the sandwich there is.<|endoftext|>`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "520875d4",
      "metadata": {
        "id": "520875d4",
        "outputId": "af9e9d69-91a2-4e51-81a1-466e54789240"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'No cement will hold hard wood.##[YODA]##>Hard wood no cement will hold.<|endoftext|>'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def formatting_func(example):\n",
        "    return f'{example[\"prompt\"]}{response_template}{example[\"completion\"]}' + tokenizer.eos_token\n",
        "\n",
        "formatting_func(prompt_yoda['train'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4da32309",
      "metadata": {
        "id": "4da32309"
      },
      "source": [
        "Now, we'll write a function that takes a prompt, formats it, and tokenizes it. It should truncate the formatted prompt according to the `max_length` argument and, optionally, pad the formatted prompt up to that length (see here the arguments for [calling](https://huggingface.co/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__) a tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c43bb9e",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "26ce4c67fade498b962fafa9bb5bfcbc"
          ]
        },
        "id": "0c43bb9e",
        "outputId": "33b2770c-d9cc-48ea-9b33-19c66972ac7b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26ce4c67fade498b962fafa9bb5bfcbc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/576 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [50256, 2949, 20534, 481, 1745, 1327, 4898, 13, 50296, 17309, 4898, 645, 20534, 481, 1745, 13, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ],
      "source": [
        "def generate_and_tokenize_prompt(prompt, max_length=128, padding=True):\n",
        "    result = tokenizer(\n",
        "        formatting_func(prompt),\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\" if padding else False,\n",
        "    )\n",
        "    return result\n",
        "\n",
        "# We'll call it WITHOUT padding first\n",
        "dataset = prompt_yoda['train'].map(lambda v: generate_and_tokenize_prompt(v, padding=False))\n",
        "dataset = dataset.remove_columns(['prompt', 'completion'])\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ba6e6e0",
      "metadata": {
        "id": "9ba6e6e0"
      },
      "source": [
        "Perhaps you're wondering where the labels are... as it turns out, the collator will take care of it. We'll be using a collator for completion only, since we're not interested in the regular English sentences that precede our special \"Yoda\" token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a8ffc51",
      "metadata": {
        "id": "7a8ffc51",
        "outputId": "557475ae-6b79-46a9-8215-0e18b9fa29f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[50295, 50295, 50295, 50256,  2949, 20534,   481,  1745,  1327,  4898,\n",
              "            13, 50296, 17309,  4898,   645, 20534,   481,  1745,    13, 50256],\n",
              "        [50256,   464,  1705,  7425,  4719,   656, 42537,  9017,    13, 50296,\n",
              "            35, 47675,   656, 42537,  9017,   262,  1705,  7425,    13, 50256]]), 'attention_mask': tensor([[0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100, 17309,  4898,   645, 20534,   481,  1745,    13, 50256],\n",
              "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "            35, 47675,   656, 42537,  9017,   262,  1705,  7425,    13, 50256]])}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from trl import DataCollatorForCompletionOnlyLM\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
        "\n",
        "dataloader_completion = DataLoader(dataset, batch_size=2, collate_fn=collator)\n",
        "next(iter(dataloader_completion))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "811b0803",
      "metadata": {
        "scrolled": false,
        "id": "811b0803",
        "outputId": "c945f811-5425-4017-a544-a7951ca70c17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "576\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_data_lengths(tokenized_train_dataset):\n",
        "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
        "    print(len(lengths))\n",
        "\n",
        "    # Plotting the histogram\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
        "    plt.xlabel('Length of input_ids')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Distribution of Lengths of input_ids')\n",
        "    plt.show()\n",
        "\n",
        "plot_data_lengths(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8c0726c",
      "metadata": {
        "id": "a8c0726c"
      },
      "source": [
        "Now, apply the `generate_and_tokenize_prompt` function to both train and validation sets using the dataset's [`map()`](https://huggingface.co/docs/datasets/en/process#map) method.\n",
        "\n",
        "You can adjust the max length to better match the observed length of the inputs (in the plot above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71156f39",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "b3d537aff6814c3cbc20031a5110faf1",
            "f462fe4d3ea84c3ca2722c7461e7ea36"
          ]
        },
        "id": "71156f39",
        "outputId": "03bc15c0-1de0-462c-fc9a-f18dd21f1dd1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3d537aff6814c3cbc20031a5110faf1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/576 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f462fe4d3ea84c3ca2722c7461e7ea36",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/144 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "max_length = 64\n",
        "tokenized_train_dataset = prompt_yoda['train'].map(lambda v: generate_and_tokenize_prompt(v, max_length))\n",
        "tokenized_val_dataset = prompt_yoda['test'].map(lambda v: generate_and_tokenize_prompt(v, max_length))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0ba8b31",
      "metadata": {
        "id": "e0ba8b31"
      },
      "source": [
        "Try tokenizing (and decoding back) one example from the training set. You should see padding tokens to the left of the sentence, an `<|endoftext|>` token signaling the beginning of the sentence (the same token is used both as BOS and EOS token), the original sentence, our special \"Yoda\" token, the translated sentence, and the EOS token at the very end.\n",
        "\n",
        "Here is an example:\n",
        "\n",
        "`'<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|endoftext|>Quench your thirst, then eat the crackers. ##[YODA]##> Quench your thirst, the crackers then eat.<|endoftext|>'`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13753530",
      "metadata": {
        "id": "13753530",
        "outputId": "4d323c4c-8193-4844-8397-668c28e43439"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|endoftext|>Quench your thirst, then eat the crackers. ##[YODA]##> Quench your thirst, the crackers then eat.<|endoftext|>'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(tokenized_train_dataset[1]['input_ids'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e82d8b0",
      "metadata": {
        "id": "4e82d8b0"
      },
      "source": [
        "## Model\n",
        "\n",
        "Now, let's load the model itself. In order to quantize it while loading it, we need an instance of [`BitAndBytesConfig`](https://huggingface.co/docs/transformers/en/main_classes/quantization#transformers.BitsAndBytesConfig). We can load it in 8-bit using the `NF4` quantization type and double quantization. The computing dtype may be `torch.float16` or - if the GPU supports it - `torch.bfloat16`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7df351eb",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "9a6fc1a165744f9b89236456e025f4ad"
          ]
        },
        "id": "7df351eb",
        "outputId": "ff20dd34-91a8-42e9-99a4-7eddc3fa63cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a6fc1a165744f9b89236456e025f4ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, trust_remote_code=True, quantization_config=bnb_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7c6f8ae",
      "metadata": {
        "id": "f7c6f8ae"
      },
      "source": [
        "Before moving forward, let's check if the embeddings layer needs resizing or not (since we have added a special token to the tokenizer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d9ad190-7305-4fa5-be65-e6684c0f4c69",
      "metadata": {
        "id": "5d9ad190-7305-4fa5-be65-e6684c0f4c69",
        "outputId": "a610158e-5373-47d8-aab9-8a82e3b6b8a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Embedding(51200, 2560), 50297)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.model.embed_tokens, len(tokenizer)\n",
        "# no need\n",
        "# model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "642187c4-d763-4d1c-9d90-33e440f611b8",
      "metadata": {
        "id": "642187c4-d763-4d1c-9d90-33e440f611b8",
        "outputId": "42e45cce-3d4d-41f4-a88f-96305d0f61f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PhiForCausalLM(\n",
              "  (model): PhiModel(\n",
              "    (embed_tokens): Embedding(51200, 2560)\n",
              "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x PhiDecoderLayer(\n",
              "        (self_attn): PhiSdpaAttention(\n",
              "          (q_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
              "          (k_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
              "          (v_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
              "          (dense): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
              "          (rotary_emb): PhiRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): PhiMLP(\n",
              "          (activation_fn): NewGELUActivation()\n",
              "          (fc1): Linear8bitLt(in_features=2560, out_features=10240, bias=True)\n",
              "          (fc2): Linear8bitLt(in_features=10240, out_features=2560, bias=True)\n",
              "        )\n",
              "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9be4de7",
      "metadata": {
        "id": "b9be4de7"
      },
      "source": [
        "How many trainable parameters are left after we load the quantized model? Which layers can still be trained? Let's check it out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "072778a0",
      "metadata": {
        "id": "072778a0"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model, verbose=False):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            if verbose:\n",
        "                print(name)\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d2b9829-1507-4352-bfd8-3df2f8521507",
      "metadata": {
        "id": "6d2b9829-1507-4352-bfd8-3df2f8521507",
        "outputId": "f15e3777-4cdc-4f5d-9b3d-fe948d89cc21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model.embed_tokens.weight\n",
            "model.layers.0.input_layernorm.weight\n",
            "model.layers.0.input_layernorm.bias\n",
            "model.layers.1.input_layernorm.weight\n",
            "model.layers.1.input_layernorm.bias\n",
            "model.layers.2.input_layernorm.weight\n",
            "model.layers.2.input_layernorm.bias\n",
            "model.layers.3.input_layernorm.weight\n",
            "model.layers.3.input_layernorm.bias\n",
            "model.layers.4.input_layernorm.weight\n",
            "model.layers.4.input_layernorm.bias\n",
            "model.layers.5.input_layernorm.weight\n",
            "model.layers.5.input_layernorm.bias\n",
            "model.layers.6.input_layernorm.weight\n",
            "model.layers.6.input_layernorm.bias\n",
            "model.layers.7.input_layernorm.weight\n",
            "model.layers.7.input_layernorm.bias\n",
            "model.layers.8.input_layernorm.weight\n",
            "model.layers.8.input_layernorm.bias\n",
            "model.layers.9.input_layernorm.weight\n",
            "model.layers.9.input_layernorm.bias\n",
            "model.layers.10.input_layernorm.weight\n",
            "model.layers.10.input_layernorm.bias\n",
            "model.layers.11.input_layernorm.weight\n",
            "model.layers.11.input_layernorm.bias\n",
            "model.layers.12.input_layernorm.weight\n",
            "model.layers.12.input_layernorm.bias\n",
            "model.layers.13.input_layernorm.weight\n",
            "model.layers.13.input_layernorm.bias\n",
            "model.layers.14.input_layernorm.weight\n",
            "model.layers.14.input_layernorm.bias\n",
            "model.layers.15.input_layernorm.weight\n",
            "model.layers.15.input_layernorm.bias\n",
            "model.layers.16.input_layernorm.weight\n",
            "model.layers.16.input_layernorm.bias\n",
            "model.layers.17.input_layernorm.weight\n",
            "model.layers.17.input_layernorm.bias\n",
            "model.layers.18.input_layernorm.weight\n",
            "model.layers.18.input_layernorm.bias\n",
            "model.layers.19.input_layernorm.weight\n",
            "model.layers.19.input_layernorm.bias\n",
            "model.layers.20.input_layernorm.weight\n",
            "model.layers.20.input_layernorm.bias\n",
            "model.layers.21.input_layernorm.weight\n",
            "model.layers.21.input_layernorm.bias\n",
            "model.layers.22.input_layernorm.weight\n",
            "model.layers.22.input_layernorm.bias\n",
            "model.layers.23.input_layernorm.weight\n",
            "model.layers.23.input_layernorm.bias\n",
            "model.layers.24.input_layernorm.weight\n",
            "model.layers.24.input_layernorm.bias\n",
            "model.layers.25.input_layernorm.weight\n",
            "model.layers.25.input_layernorm.bias\n",
            "model.layers.26.input_layernorm.weight\n",
            "model.layers.26.input_layernorm.bias\n",
            "model.layers.27.input_layernorm.weight\n",
            "model.layers.27.input_layernorm.bias\n",
            "model.layers.28.input_layernorm.weight\n",
            "model.layers.28.input_layernorm.bias\n",
            "model.layers.29.input_layernorm.weight\n",
            "model.layers.29.input_layernorm.bias\n",
            "model.layers.30.input_layernorm.weight\n",
            "model.layers.30.input_layernorm.bias\n",
            "model.layers.31.input_layernorm.weight\n",
            "model.layers.31.input_layernorm.bias\n",
            "model.final_layernorm.weight\n",
            "model.final_layernorm.bias\n",
            "lm_head.weight\n",
            "lm_head.bias\n",
            "trainable params: 262364160 || all params: 2779683840 || trainable%: 9.438633136061977\n"
          ]
        }
      ],
      "source": [
        "print_trainable_parameters(model, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6eed82b8",
      "metadata": {
        "id": "6eed82b8"
      },
      "source": [
        "## LoRA\n",
        "\n",
        "Quantization makes the model smaller to load, but we still need LoRA to make training faster.\n",
        "\n",
        "So, we need to create an instance of [`LoraConfig`](https://huggingface.co/docs/peft/main/en/developer_guides/quantization#loraconfig). You need to choose a rank (`r`), the alpha multiplier (`lora_alpha`), the target modules that will be modified by LoRA (`target_modules`), and - optionally - other modules that should be trained and saved (`modules_to_save`).\n",
        "\n",
        "These extra modules may include layer norm and embeddings modules, for example. Including these modules may deliver better performance but it comes at the cost of not being able to merge multiple adapters together later.\n",
        "\n",
        "Next, you can use the configuration to get the modified model using `get_peft_model()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ae5103c",
      "metadata": {
        "id": "2ae5103c",
        "outputId": "09229afb-8431-4e5e-f286-d27e3a220cf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 146,519,040 || all params: 2,926,202,880 || trainable%: 5.0071\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"dense\"\n",
        "        \"fc1\", \"fc2\",\n",
        "        \"lm_head\",\n",
        "    ],\n",
        "    modules_to_save=['layernorm', 'embed_tokens'],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,  # Conventional\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(model, config)\n",
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72b5abd5-5af2-4e3e-b23f-386fff972db9",
      "metadata": {
        "id": "72b5abd5-5af2-4e3e-b23f-386fff972db9",
        "outputId": "e1dd09f8-771d-480b-a337-10fea9f66672"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "base_model.model.model.embed_tokens.modules_to_save.default.weight\n",
            "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.0.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.0.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.0.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.0.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.1.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.1.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.1.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.1.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.2.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.2.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.2.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.2.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.3.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.3.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.3.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.3.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.4.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.4.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.4.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.4.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.5.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.5.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.5.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.5.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.6.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.6.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.6.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.6.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.7.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.7.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.7.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.7.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.8.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.8.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.8.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.8.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.9.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.9.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.9.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.9.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.10.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.10.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.10.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.10.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.11.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.11.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.11.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.11.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.12.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.12.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.12.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.12.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.13.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.13.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.13.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.13.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.14.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.14.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.14.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.14.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.15.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.15.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.15.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.15.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.16.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.16.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.16.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.16.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.17.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.17.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.17.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.17.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.18.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.18.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.18.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.18.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.19.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.19.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.19.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.19.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.20.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.20.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.20.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.20.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.21.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.21.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.21.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.21.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.22.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.22.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.22.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.22.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.23.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.23.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.23.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.23.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.24.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.24.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.24.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.24.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.25.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.25.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.25.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.25.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.26.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.26.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.26.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.26.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.27.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.27.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.27.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.27.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.28.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.28.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.28.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.28.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.29.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.29.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.29.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.29.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.30.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.30.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.30.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.30.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.31.mlp.fc2.lora_A.default.weight\n",
            "base_model.model.model.layers.31.mlp.fc2.lora_B.default.weight\n",
            "base_model.model.model.layers.31.input_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.layers.31.input_layernorm.modules_to_save.default.bias\n",
            "base_model.model.model.final_layernorm.modules_to_save.default.weight\n",
            "base_model.model.model.final_layernorm.modules_to_save.default.bias\n",
            "base_model.model.lm_head.lora_A.default.weight\n",
            "base_model.model.lm_head.lora_B.default.weight\n",
            "trainable params: 146519040 || all params: 2926202880 || trainable%: 5.007138807819095\n"
          ]
        }
      ],
      "source": [
        "print_trainable_parameters(peft_model, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c39202a",
      "metadata": {
        "id": "3c39202a"
      },
      "source": [
        "## Training\n",
        "\n",
        "Before actually training the model, we have to configure its training arguments. Hugging Face's `TrainingArguments` is very thorough and comprehensive, so we're providing suggested arguments right away.\n",
        "\n",
        "It is important to notice that:\n",
        "- it uses a paged 8-bit optimizer in order to save memory\n",
        "- it uses gradient accumulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5571db2b",
      "metadata": {
        "id": "5571db2b"
      },
      "outputs": [],
      "source": [
        "# Some Environment Setup\n",
        "OUTPUT_DIR = \"./results/yoda/\" # the path to the output directory; where model checkpoints will be saved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02187851",
      "metadata": {
        "id": "02187851"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "\n",
        "training_args = transformers.TrainingArguments(\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        warmup_steps=1,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        auto_find_batch_size=True,\n",
        "        max_steps=1000,\n",
        "        learning_rate=2.5e-5,        # Want a small lr for finetuning\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        logging_steps=25,            # When to start reporting loss\n",
        "        logging_dir=\"./logs\",        # Directory for storing logs\n",
        "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
        "        save_steps=500,              # Save checkpoints every 50 steps\n",
        "        eval_strategy=\"steps\",       # Evaluate the model every logging step\n",
        "        eval_steps=25,               # Evaluate and save checkpoints every 50 steps\n",
        "        do_eval=True,                # Perform evaluation at the end of training\n",
        "        report_to=\"none\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6782df9",
      "metadata": {
        "id": "e6782df9"
      },
      "source": [
        "The `Trainer` object needs:\n",
        "- a model (`model` arg)\n",
        "- a training set (`train_dataset` arg)\n",
        "- an (optional) validation set (`eval_dataset` arg)\n",
        "- the training arguments (`args` arg)\n",
        "- a data collator (`data_collator` arg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7049e7f8",
      "metadata": {
        "id": "7049e7f8"
      },
      "source": [
        "Training may take around 10 minutes in an RTX 3090. In Colab's free version, it will take much longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4acbee45",
      "metadata": {
        "id": "4acbee45",
        "outputId": "d1106383-d570-43e8-e2d4-91edf7a3a445"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 09:50, Epoch 13/14]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>3.162800</td>\n",
              "      <td>2.729766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.532500</td>\n",
              "      <td>1.987815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>1.794400</td>\n",
              "      <td>1.543532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.519800</td>\n",
              "      <td>1.296836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>1.264500</td>\n",
              "      <td>1.110637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.018600</td>\n",
              "      <td>0.994319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.922400</td>\n",
              "      <td>0.832627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.701000</td>\n",
              "      <td>0.685616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.636800</td>\n",
              "      <td>0.621049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.509300</td>\n",
              "      <td>0.565084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.529300</td>\n",
              "      <td>0.552678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.444700</td>\n",
              "      <td>0.520075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>0.424800</td>\n",
              "      <td>0.510664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.422900</td>\n",
              "      <td>0.498617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>0.428100</td>\n",
              "      <td>0.478401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.378300</td>\n",
              "      <td>0.471837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>425</td>\n",
              "      <td>0.362100</td>\n",
              "      <td>0.462808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.399300</td>\n",
              "      <td>0.456574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>475</td>\n",
              "      <td>0.324600</td>\n",
              "      <td>0.444118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.369100</td>\n",
              "      <td>0.439637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>525</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.441958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.343100</td>\n",
              "      <td>0.441537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>0.293500</td>\n",
              "      <td>0.432965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.317100</td>\n",
              "      <td>0.435015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>625</td>\n",
              "      <td>0.291100</td>\n",
              "      <td>0.432573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.293800</td>\n",
              "      <td>0.431924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>675</td>\n",
              "      <td>0.300400</td>\n",
              "      <td>0.430797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.316300</td>\n",
              "      <td>0.432331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>725</td>\n",
              "      <td>0.273900</td>\n",
              "      <td>0.424429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.286400</td>\n",
              "      <td>0.424575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>775</td>\n",
              "      <td>0.275100</td>\n",
              "      <td>0.424179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.303400</td>\n",
              "      <td>0.421751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>825</td>\n",
              "      <td>0.281000</td>\n",
              "      <td>0.418980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.272100</td>\n",
              "      <td>0.415066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>875</td>\n",
              "      <td>0.261500</td>\n",
              "      <td>0.414659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.272900</td>\n",
              "      <td>0.413970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>925</td>\n",
              "      <td>0.271300</td>\n",
              "      <td>0.411701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.273000</td>\n",
              "      <td>0.413010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>975</td>\n",
              "      <td>0.259900</td>\n",
              "      <td>0.414200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.270800</td>\n",
              "      <td>0.411995</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/micromamba/envs/python_310/lib/python3.10/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/opt/micromamba/envs/python_310/lib/python3.10/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1000, training_loss=0.5985516076087951, metrics={'train_runtime': 591.2044, 'train_samples_per_second': 13.532, 'train_steps_per_second': 1.691, 'total_flos': 8183988879360000.0, 'train_loss': 0.5985516076087951, 'epoch': 13.88888888888889})"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=peft_model,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    args=training_args,\n",
        "    data_collator=collator\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18ee1c98",
      "metadata": {
        "id": "18ee1c98"
      },
      "source": [
        "After 1,000 steps, training loss should be around 0.3. So, we save the trained model to disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc8aa532",
      "metadata": {
        "id": "bc8aa532",
        "outputId": "4eef4930-6ed1-4824-ad6a-0523097dd18e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/micromamba/envs/python_310/lib/python3.10/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
          ]
        }
      ],
      "source": [
        "model_ckpt = OUTPUT_DIR + \"/stop\"\n",
        "\n",
        "trainer.save_model(model_ckpt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d310dbb",
      "metadata": {
        "id": "7d310dbb"
      },
      "source": [
        "## Reloading the Model\n",
        "\n",
        "Now, let's reload the trained adapter we have just saved. Remember, it only saves a partial model, so we still need the (quantized) base model.\n",
        "\n",
        "We can use [`PeftModel.from_pretrained()`](https://huggingface.co/docs/peft/en/package_reference/peft_model#peft.PeftModel.from_pretrained) method to load the fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!gdown 19z2Jn7N-Gek0z1Rhyyh28XDPoIt264vN\n",
        "#!unzip yoda_adapter.zip -d ./results/yoda/stop"
      ],
      "metadata": {
        "id": "gzkBRfrECsAr"
      },
      "id": "gzkBRfrECsAr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f139b83b-1885-499b-af8a-9fdbd474d3b8",
      "metadata": {
        "id": "f139b83b-1885-499b-af8a-9fdbd474d3b8"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "fine_tuned_model = PeftModel.from_pretrained(model, model_ckpt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "290c58bf",
      "metadata": {
        "id": "290c58bf"
      },
      "source": [
        "Now, let's try out our model!\n",
        "\n",
        "First, we'll \"forget\" the response template and see how the model reacts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f0884df-f9ab-48c5-b367-dc6358c037c4",
      "metadata": {
        "id": "8f0884df-f9ab-48c5-b367-dc6358c037c4",
        "outputId": "c19efa09-bae0-4164-e2d0-0e2a562fb48d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Luke, I am your father!<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "eval_prompt = \"Luke, I am your father!\"\n",
        "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "fine_tuned_model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(fine_tuned_model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.1)[0], skip_special_tokens=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a77dd238",
      "metadata": {
        "id": "a77dd238"
      },
      "source": [
        "Nothing happened... what if we add the proper response template (`##[YODA]##>`)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d417267a-16ba-4518-b47d-55af2eb2498f",
      "metadata": {
        "id": "d417267a-16ba-4518-b47d-55af2eb2498f",
        "outputId": "01fb1020-4717-4734-be99-dc66cb5074f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am your father! ##[YODA]##> Your father I am.<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "eval_prompt = \"I am your father!\"\n",
        "model_input = tokenizer(eval_prompt+response_template, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "fine_tuned_model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(fine_tuned_model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.1)[0], skip_special_tokens=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcbc7901",
      "metadata": {
        "id": "fcbc7901"
      },
      "source": [
        "OK, that's more like it! We got a Yoda-like sentence back!\n",
        "\n",
        "Let's write a function that handles all the boilerplate for us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9378e164-2671-4351-9dbc-b05c66e59f87",
      "metadata": {
        "id": "9378e164-2671-4351-9dbc-b05c66e59f87"
      },
      "outputs": [],
      "source": [
        "def generate(model, tokenizer, prompt, response_template=\"\", max_new_tokens=100):\n",
        "    tokenized_input = tokenizer(prompt+response_template, return_tensors=\"pt\")\n",
        "    input_ids = tokenized_input[\"input_ids\"].cuda()\n",
        "\n",
        "    model.eval()\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        num_beams=3,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        repetition_penalty=1.1,\n",
        "        do_sample=True, top_p=0.9,temperature=0.95,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    output = tokenizer.batch_decode(generation_output, skip_special_tokens=False)[0]\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f06d644",
      "metadata": {
        "id": "2f06d644"
      },
      "source": [
        "Now, let's see our new function in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "961b4634-71ea-4469-82fb-cce2837246c3",
      "metadata": {
        "id": "961b4634-71ea-4469-82fb-cce2837246c3",
        "outputId": "6c3fcb36-caed-4405-97a8-ea264db438c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'The Force is strong in this one. ##[YODA]##> Strong in this one the force is.<|endoftext|>'"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate(fine_tuned_model, tokenizer, 'The Force is strong in this one.', response_template, max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccaf6ac0-31fc-41eb-8fbc-bb48103c21ca",
      "metadata": {
        "id": "ccaf6ac0-31fc-41eb-8fbc-bb48103c21ca",
        "outputId": "f59ea1bf-4033-475c-fad3-248bc88188ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'I am coming home. ##[YODA]##> Coming home I am. Yes, hrrmmm.<|endoftext|>'"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate(fine_tuned_model, tokenizer, 'I am coming home.', response_template, max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "513af2d4-80dd-4b7a-920e-e55a56667f30",
      "metadata": {
        "id": "513af2d4-80dd-4b7a-920e-e55a56667f30",
        "outputId": "fd0c61c9-aa03-49ff-b588-7b2e94552e5f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'The glow deepened in the eyes of the sweet girl. ##[YODA]##> Deepened in the eyes of the sweet girl the glow was.<|endoftext|>'"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample = prompt_yoda['test'][1]\n",
        "generate(fine_tuned_model, tokenizer, sample['prompt'], response_template, max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6959363",
      "metadata": {
        "id": "c6959363"
      },
      "source": [
        "Finally, we can *disable* the LoRA adapter we trained to see how the base model reacts to the sample sentence (with and without the response template):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bbcde4f-7e27-4ba1-973e-13ab57d3d556",
      "metadata": {
        "scrolled": false,
        "id": "1bbcde4f-7e27-4ba1-973e-13ab57d3d556",
        "outputId": "0e0b6e37-2bab-4562-a52a-6d1a71adb114"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The glow deepened in the eyes of the sweet girl.\n",
            "\n",
            "\"I know,\" she said, her voice barely above a whisper. \"It's like we're two pieces of a puzzle that were meant to fit together.\"\n",
            "\n",
            "The man smiled, his heart swelling with love.\n",
            "\n",
            "\"Exactly,\" he said. \"And I'm so grateful that we finally found each other.\"\n",
            "\n",
            "They sat in silence for a few moments, lost in their own thoughts.\n",
            "\n",
            "\"You know,\" the man said, breaking the silence. \"I\n",
            "The glow deepened in the eyes of the sweet girl. ##[YODA]##> s words had resonated with her, and she felt a newfound sense of purpose. She was determined to make a difference in the world, and she was ready to take on the challenge.\n",
            "\n",
            "Theory of Mind Exercises:\n",
            "\n",
            "1. What was the mood of the sweet girl when she first entered the shop?\n",
            "\n",
            "Answer: The sweet girl was likely feeling a sense of anticipation and excitement when she first entered the shop. She was eager to learn more about the\n"
          ]
        }
      ],
      "source": [
        "with fine_tuned_model.disable_adapter():\n",
        "    print(generate(fine_tuned_model, tokenizer, sample['prompt']))\n",
        "    print(generate(fine_tuned_model, tokenizer, sample['prompt'], response_template))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1248c637-ab9f-4d25-8859-b98ee7285cd9",
      "metadata": {
        "id": "1248c637-ab9f-4d25-8859-b98ee7285cd9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}